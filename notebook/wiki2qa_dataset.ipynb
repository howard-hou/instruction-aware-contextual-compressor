{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13c52c9-d093-4800-ad05-efcf9cf48967",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58709ad-e2cd-4cb4-88f9-72b991aa2126",
   "metadata": {},
   "source": [
    "### 1.1 Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c67427-5804-47b0-952b-214052bd5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "!pip install sentence_transformers datasets\n",
    "# if you have gpu, install faiss gpt\n",
    "!pip install faiss-gpu\n",
    "# else instlal faiss cpu\n",
    "# !pip install faiss-cpu\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89ac1c-bf6c-48aa-843e-5ec2567bab5a",
   "metadata": {},
   "source": [
    "### 1.2 Setup Retriever\n",
    "here we use m3e-base as the chinese Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de261818-aad3-4a29-b832-6ad2a74c48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/howard/miniconda3/envs/torch1.13/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/howard/miniconda3/envs/torch1.13 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/local/cuda/lib64: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('{\"locale\"'), PosixPath('{\"*\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/6c3e3dba23e8fadc360aed75ce363ba185c49794\",\"_corruptedFile\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('\"fe9960b3a97de5d66c86c362d68dbccd.zh-cn\",\"_translationsConfigFile\"'), PosixPath('true}'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"zh-cn\",\"osLocale\"')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: * Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one\n",
      "Embedding: (768,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('moka-ai/m3e-base')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = [\n",
    "    '* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem',\n",
    "    '* Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练',\n",
    "    '* Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one'\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2db9c1-8648-4e1e-abda-c96d114beccf",
   "metadata": {},
   "source": [
    "## 2. Preprocess QA data\n",
    "### 2.1 clean qa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ce3086-1239-45fc-a0b8-bfbbc237b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_special_char(pattern, text, src=\"\\n\\n\", tgt=\"\\n\"):\n",
    "    while True:\n",
    "        i = re.search(pattern, text)\n",
    "        if i is None:\n",
    "            break\n",
    "        s, e = i.span()\n",
    "        # split text into 3 parts\n",
    "        q, a, b = text[:s], text[s:e], text[e:]\n",
    "        a = a.replace(src, tgt)\n",
    "        # merge 3 parts\n",
    "        text = q + a + b\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    # 处理特殊情况：Q和A分隔符也是\\n\\n, 替换为\\n\n",
    "    text = replace_special_char(\"Q: .*?\\n\\nA:\", text)\n",
    "    text = replace_special_char(\"Q[0-9]+: .*?\\n\\nA[0-9]+:\", text)\n",
    "    text = replace_special_char(\"[0-9]+\\. .*?\\n\\n答：\", text)\n",
    "    text = replace_special_char(\"[0-9]+\\. .*?\\n\\n\", text)\n",
    "    # 处理特殊情况：QA和QA分隔符是\\n, 替换为\\n\\n\n",
    "    text = replace_special_char('(?<!\\n)\\n\\d+\\.', text, src=\"\\n\", tgt=\"\\n\\n\")\n",
    "    qa_pairs = [qa for qa in text.split('\\n\\n') if '\\n' in qa]\n",
    "    return qa_pairs\n",
    "\n",
    "def clean_qa(q, a):\n",
    "    # Remove \"1.\" at the beginning of the sentence\n",
    "    cleaned_q = re.sub(r'^\\d+\\.\\s+', '', q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^Q\\d+:\\s+', '', cleaned_q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^Q:\\s+', '', cleaned_q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^> \\d+\\. ', '', cleaned_q)\n",
    "    \n",
    "    # Remove \"答：\" at the beginning of the sentence\n",
    "    cleaned_a = re.sub(r'^答：', '', a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^A\\d+:\\s+', '', cleaned_a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^A:\\s+', '', cleaned_a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^-\\s+', '', cleaned_a)\n",
    "    cleaned_a = re.sub(r'^回答：', '', cleaned_a)\n",
    "    \n",
    "    return cleaned_q.strip(), cleaned_a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb6904a-f1cc-4d53-a5c6-868ebb6ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_to_12_length(input_string):\n",
    "    # Create an MD5 hash object\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # Convert the input string to bytes and update the hash object\n",
    "    md5_hash.update(input_string.encode('utf-8'))\n",
    "    \n",
    "    # Get the hexadecimal representation of the hash\n",
    "    hashed_string = md5_hash.hexdigest()\n",
    "    \n",
    "    # Truncate the hash to 12 characters\n",
    "    truncated_hash = hashed_string[:12]\n",
    "    \n",
    "    return truncated_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b7ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_ngram(text, n=2):\n",
    "    # 创建 n-gram 列表\n",
    "    ngrams = [\"\".join(text[i:i+n]) for i in range(len(text) - n + 1)]\n",
    "    # 统计 n-gram 出现次数\n",
    "    ngram_counter = Counter(ngrams)\n",
    "    # 计算总单词数\n",
    "    total_words = len(ngrams)\n",
    "    # 计算每个 n-gram 的频率\n",
    "    ngram_freq = {ngram: ngram_counter[ngram] / total_words for ngram in ngram_counter}\n",
    "    return ngram_freq\n",
    "\n",
    "def is_repeated_text(text, threshold=0.1, n=2):\n",
    "    # 计算词组频率\n",
    "    ngram_freq = count_ngram(text, n)\n",
    "    # 如果 max(ngram_freq.values()) 大于阈值，则返回False\n",
    "    return max(ngram_freq.values()) > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3637f787-7d1e-492e-a092-5317df844186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filter, qa pairs num: 1794483\n",
      "after filter, qa pairs num: 1761612\n",
      "drop qa pairs num ratio: 0.018317810756635754\n",
      "max ans len in keep: 283\n",
      "min ans len in keep: 9\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "wikiqa_dir = Path(\"data/wikiqa\")\n",
    "wikiqa_dir.mkdir(exist_ok=True)\n",
    "qa_input_path = \"data/raw_data/wikipedia-cn-20230720-ref2qa_all.json\"\n",
    "qa_ouput_path = \"data/wikiqa/wikipedia-cn-20230720-qapairs_all.json\"\n",
    "document_path = \"data/wikiqa/wikipedia-cn-20230720-documents_all.json\"\n",
    "qa_data = json.load(open(qa_input_path))\n",
    "\n",
    "# 1. build documents\n",
    "documents = []\n",
    "for line in qa_data:\n",
    "    doc = line[\"input\"].strip()\n",
    "    docid = hash_to_12_length(doc)\n",
    "    docid2doc = dict(docid=docid, document=doc, source=line[\"source\"])\n",
    "    documents.append(docid2doc)\n",
    "json.dump(documents, open(document_path, \"w\"), ensure_ascii=False)\n",
    "\n",
    "# 2. build qa pairs\n",
    "qa_pairs_with_docid = []\n",
    "ans_lens = []\n",
    "for line in qa_data:\n",
    "    qa_pairs = split_text(line[\"output\"])\n",
    "    doc = line[\"input\"].strip()\n",
    "    docid = hash_to_12_length(doc)\n",
    "    for i, qa_pair in enumerate(qa_pairs):\n",
    "        qa_pair = qa_pair.strip().split(\"\\n\")\n",
    "        q, a = qa_pair[0], \"\\n\".join(qa_pair[1:])\n",
    "        q, a = clean_qa(q, a)\n",
    "        if not q or not a:\n",
    "            # 最后一个qa_pair可能是空的\n",
    "            if i == len(qa_pairs) - 1:\n",
    "                continue\n",
    "            else:\n",
    "                print(q, a, qa_pair) # make sure q and a are not empty\n",
    "        ans_lens.append(len(a))\n",
    "        item = dict(question=q, answer=a, docid=docid)\n",
    "        qa_pairs_with_docid.append(item)\n",
    "        \n",
    "# 3. statistics and filter\n",
    "print(\"before filter, qa pairs num:\", len(qa_pairs_with_docid))\n",
    "qa_pairs_with_docid_keep = []\n",
    "qa_pairs_with_docid_drop = []\n",
    "ans_len_percentile99 = np.percentile(ans_lens, 99)\n",
    "ans_len_percentile1 = np.percentile(ans_lens, 1)\n",
    "ans_len_percentile95 = np.percentile(ans_lens, 95)\n",
    "for item in qa_pairs_with_docid:\n",
    "    if (len(item[\"answer\"]) >= ans_len_percentile1 and \n",
    "        len(item[\"answer\"]) <= ans_len_percentile99):\n",
    "\n",
    "        if (len(item[\"answer\"]) > ans_len_percentile95 and \n",
    "            is_repeated_text(item[\"answer\"], threshold=0.10)):\n",
    "\n",
    "            qa_pairs_with_docid_drop.append(item)\n",
    "        else:\n",
    "            qa_pairs_with_docid_keep.append(item)\n",
    "    else:\n",
    "        qa_pairs_with_docid_drop.append(item)\n",
    "print(\"after filter, qa pairs num:\", len(qa_pairs_with_docid_keep))\n",
    "print(\"drop qa pairs num ratio:\", len(qa_pairs_with_docid_drop) / len(qa_pairs_with_docid))\n",
    "print(\"max ans len in keep:\", max([len(item[\"answer\"]) for item in qa_pairs_with_docid_keep]))\n",
    "print(\"min ans len in keep:\", min([len(item[\"answer\"]) for item in qa_pairs_with_docid_keep]))\n",
    "\n",
    "json.dump(qa_pairs_with_docid_keep, open(qa_ouput_path, \"w\"), ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09605b00-4269-45d4-986a-6d050a77fdbe",
   "metadata": {},
   "source": [
    "### 2.2 build faiss index for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5382346d-a200-4a51-bcf7-d6fd13752560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'document', 'source'],\n",
      "    num_rows: 254338\n",
      "})\n",
      "{\n",
      "  \"docid\": \"1e13ffb6871c\",\n",
      "  \"document\": \"路易斯·萨恩斯·佩尼亚·达比拉（Luis Sáenz Peña Dávila，12月4日) ，律师和阿根廷总统（1890年—1892年）。\\n他从布宜诺斯艾利斯大学法律系毕业，参加1860年宪法汇编。他是全国代理和参议员之一。1882年他占有在布宜诺斯艾利斯省最高法院的一个位子。后他被雇用作为省银行的主席，法律学院的主任和在教育委员会一个成员。\\n1892年任总统，1895年1月23日他对国会提出辞职并被接受，政府转到何塞·埃瓦里斯托·乌里武鲁将军手中，他1898年完成任期。\",\n",
      "  \"source\": \"wikipedia.zh2307\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# document_dataset = load_dataset(\"json\", data_files=document_path)\n",
    "document_dataset = Dataset.from_list(documents)\n",
    "print(document_dataset)\n",
    "print(json.dumps(document_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb7ca00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# if doc embedding has been processed, then load here\n",
    "document_dataset.load_faiss_index('doc_embedding', 'data/raw_data/wiki_doc_embedding.faiss')\n",
    "document_dataset_with_emb = document_dataset\n",
    "print(document_dataset_with_emb.is_index_initialized(\"doc_embedding\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd4eb2c-6f7e-4888-bcd9-30c892395a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686bb4e1451641eaa791fb9ea57ee26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2849ebcba34d5290fe717d53ee1f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "document_dataset_with_emb = document_dataset.map(\n",
    "    lambda example: {'doc_embedding': model.encode(example[\"document\"])}, \n",
    "    batched=True\n",
    ")\n",
    "# 模型index在cpu，设置device可以把index放到对应gpu，#device is the index of the GPU to use !!!但有非常慢\n",
    "document_dataset_with_emb.add_faiss_index(column='doc_embedding')\n",
    "document_dataset_with_emb.save_faiss_index('doc_embedding', \n",
    "                                           'data/raw_data/wiki_doc_embedding.faiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feeaa53-2db3-4f80-9adf-506e9736a072",
   "metadata": {},
   "source": [
    "### 2.3 encode questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "809787f1-d6dc-4cbe-b730-311c66f23bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid'],\n",
      "    num_rows: 1761612\n",
      "})\n",
      "{'question': '路易斯·萨恩斯·佩尼亚·达比拉是谁？', 'answer': '路易斯·萨恩斯·佩尼亚·达比拉是阿根廷总统和律师，出生于1862年。', 'docid': '1e13ffb6871c'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f365a0f6c274cafaf66a2d7ea68b82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1761612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qa_dataset = load_dataset(\"json\", data_files=\"wikipedia-cn-20230720-qapairs_10k.json\")\n",
    "qa_dataset = Dataset.from_list(qa_pairs_with_docid_keep)\n",
    "print(qa_dataset)\n",
    "print(qa_dataset[0])\n",
    "# \n",
    "qa_dataset_with_emb = qa_dataset.map(\n",
    "    lambda example: {'question_embedding': model.encode(example[\"question\"])}, \n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50022e28-58cf-4fa8-9052-39454a42e206",
   "metadata": {},
   "source": [
    "## 3. Build dataset\n",
    "### 3.1 retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d157ff-75e0-48b8-b636-71a681430c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_topk_documents(example):\n",
    "    topk = 100\n",
    "    ques_embedding = np.array(example[\"question_embedding\"], dtype=np.float32)\n",
    "    scores, retrieved_examples = document_dataset_with_emb.get_nearest_examples('doc_embedding', ques_embedding, k=topk)\n",
    "    example[\"retrieved_docids\"] = retrieved_examples[\"docid\"]\n",
    "    example[\"retrieved_doc_scores\"] = scores.tolist()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ebd94a-8301-4fb2-bbd7-4144ef4b280b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1175d45639b84810b0ea108972370d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=64):   0%|          | 0/1761612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据硬件，调整num_proc\n",
    "qa_dataset_with_retrieval = qa_dataset_with_emb.map(retrieve_topk_documents, \n",
    "                                                    num_proc=64,\n",
    "                                                    remove_columns=[\"question_embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc74bd4-0869-4ecb-89de-1997acc308b0",
   "metadata": {},
   "source": [
    "## 4. Evaluate\n",
    "### 4.1 Evaluate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87f3f3b-bff8-4ef7-abe0-d1785b585666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    for pred, true_label in zip(predictions, true_labels):\n",
    "        if true_label in pred[:topk]:\n",
    "            num_correct += 1\n",
    "    \n",
    "    top1_accuracy = num_correct / len(predictions)\n",
    "    return top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "525e4540-8e9a-41b9-af90-47a2e89c7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1761612/1761612 [06:57<00:00, 4214.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "questions, predictions, targets = [], [], []\n",
    "for s in tqdm(qa_dataset_with_retrieval):\n",
    "    questions.append(s[\"question\"])\n",
    "    predictions.append(s[\"retrieved_docids\"])\n",
    "    targets.append(s[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45558dc7-44fc-42df-a6f6-cc9c0b3b118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1: 0.7622456023233266\n",
      "top3: 0.8342580545545785\n",
      "top5: 0.851998623987575\n",
      "top10: 0.8704555827276381\n",
      "top20: 0.8856388353394504\n",
      "top50: 0.9032709813511716\n",
      "top100: 0.9158253917434713\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "    topk_acc = compute_topk_accuracy(predictions, targets, topk=k)\n",
    "    print(f\"top{k}:\", topk_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e512d7-7fa4-40d2-97c1-8c1e0380956c",
   "metadata": {},
   "source": [
    "### 4.2 check badcase\n",
    "因为我觉得top100都无法召回的问题，可能是低质量问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fea3c1b4-f744-4b60-90f2-e93baa4e2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"释道融的师父喜欢他的什么特点？\",\n",
      "        \"释道融的师父喜欢他的精神、风采。\"\n",
      "    ],\n",
      "    [\n",
      "        \"什么是水库诱发地震的震中？\",\n",
      "        \"水库诱发地震的震中多在库底和水库边缘。\"\n",
      "    ],\n",
      "    [\n",
      "        \"《如果云知道》的专辑文案中，哪首歌曲获得了金曲奖的最佳作词奖？\",\n",
      "        \"《如果云知道》的专辑文案中，《如果云知道》获得了金曲奖的最佳作词奖。\"\n",
      "    ],\n",
      "    [\n",
      "        \"Who is Gingerbread?\",\n",
      "        \"Gingerbread is a band founded by Hong Kong singer Leung Wing Fook under the identity of \\\"Ming Fuk Yi\\\".\"\n",
      "    ],\n",
      "    [\n",
      "        \"双标紫斑蝶的分布范围是哪些地区？\",\n",
      "        \"双标紫斑蝶广泛分布于南亚、东南亚、澳洲、新几内亚等地。台湾地区于本岛中海拔地区可见，多以特有亚种归类。\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "# check qa quality\n",
    "index = [randrange(len(qa_dataset_with_retrieval)) for i in range(5)]\n",
    "small_dataset = qa_dataset_with_retrieval.select(index).remove_columns([\"retrieved_docids\"])\n",
    "print(json.dumps([(i[\"question\"], i[\"answer\"]) for i in small_dataset], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbf2a97-543b-4bf8-848c-1a8744cd0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_badcase(questions, predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    bad_cases = []\n",
    "    for ques, pred, true_label in zip(questions, predictions, true_labels):\n",
    "        if true_label not in pred[:topk]:\n",
    "            bad_cases.append(ques)\n",
    "    return bad_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65d8a741-7105-42c5-9f4d-7b99f8a5e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148283 ['林肯是如何成为律师和议员的？', '与上一张商贩单曲《This Silence Is Mine/你与SciencE》相隔多久？', '歌词网站「歌Net」于作品发行前先行揭露了哪首歌曲的歌词？', '官方网站公开了哪首歌曲的音乐录像带无声制作花絮？', '谁是埃塞俄比亚运动员德斯塔·阿斯杰多姆？', '谁是苏联跳高运动员谢尔希·谢纽科夫？', '谁是波兰政治家彼得·雅罗谢维奇？', '谁是美国高尔夫球手奇克哈伯特？', '谁是沙特阿拉伯叛教者巴巴拉·麦克林托克？', '谁是美国遗传学家约翰尼·莫蒂默？']\n"
     ]
    }
   ],
   "source": [
    "top100_bad_case = check_badcase(questions, predictions, targets, topk=100)\n",
    "print(len(top100_bad_case), top100_bad_case[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c23001c-bc9e-48ce-982d-b97d1430577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20931 ['节目的收官演唱会叫什么名字？', '如果投手因为球队的守备失误而失分的话，是否列入责失分当中？', '该电影获得了哪些奖项和提名？', 'What did Roger Taylor study at university?', '该公约的目的是什么？', '该公约的条款有哪些？', '勇者斗恶龙系列在全球出货量是多少？', '苏高利条约是否有效？', '宁德时代的发展历史是怎样的？', '宁德时代被哪个国家选为“国家名片”？']\n"
     ]
    }
   ],
   "source": [
    "top50_bad_case = check_badcase(questions, predictions, targets, topk=50)\n",
    "top50_bad_case_diff = [q for q in top50_bad_case if q not in top100_bad_case]\n",
    "print(len(top50_bad_case_diff), top50_bad_case_diff[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cce408cd-321a-4833-b89c-fac067ace888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201460 30053 ['E组的比赛是在哪一天举行的？', 'What happened during the 1997-1998 civil war in the Republic of Congo?', '什么是飓风？', '松岛的陆地面积是多少？', \"What is Au Kin Yee's affiliation with the Hong Kong film industry?\", 'Who is the current principal of YY3?', '什么是最佳化问题？', '少女的判决是什么？', '该集的首播前，HBO在网络上播出了什么？', '2002年2月17日中国冬奥会历史上的第一枚金牌是谁获得的？']\n"
     ]
    }
   ],
   "source": [
    "top20_bad_case = check_badcase(questions, predictions, targets, topk=20)\n",
    "top20_bad_case_diff = [q for q in top20_bad_case if q not in top50_bad_case]\n",
    "top20_bad_case_diff_sampled = [top20_bad_case_diff[randrange(len(top20_bad_case_diff))] for i in range(10)]\n",
    "print(len(top20_bad_case), len(top20_bad_case_diff), top20_bad_case_diff_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a1567-d7f9-481f-ab6a-a675d26840ab",
   "metadata": {},
   "source": [
    "##### 观察： top20未召回的问题，确实不少是低质量问题，也有一些是比较难的问题。也是召回系统的问题，一些实体无法正确召回"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a9397-e774-4c94-ad86-8c0623f671c2",
   "metadata": {},
   "source": [
    "## 5. Filtering\n",
    "保留top20内能召回正确文档的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc4f3af-6801-4932-bede-474d873afdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b923e2f2f8d452cabb235cd7c6773b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1761612 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "    num_rows: 1558310\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_filtered = qa_dataset_with_retrieval.filter(lambda x: x[\"question\"] not in top20_bad_case)\n",
    "print(qa_dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb59b5",
   "metadata": {},
   "source": [
    "过滤重复问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c39f532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('祁汉是哪里人？', 21),\n",
       " ('好小子是什么游戏？', 19),\n",
       " ('小串成重的父亲是谁？', 18),\n",
       " ('陈横是谁的部将？', 18),\n",
       " ('常宁市有哪些文物保护单位？', 16),\n",
       " ('剑南道的治所在哪个县？', 16),\n",
       " ('陈志钊在哪个足球俱乐部效力过？', 16),\n",
       " ('李琰的女儿适谁？', 15),\n",
       " ('洋流的分类有哪些？', 15),\n",
       " ('吕壹为什么被处死？', 14)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(qa_dataset_filtered[\"question\"])\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ec61ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
       "    num_rows: 1524340\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qa_df = pd.DataFrame(qa_dataset_filtered)\n",
    "qa_df.drop_duplicates(subset=['question'], inplace=True)\n",
    "qa_dataset_dedup = Dataset.from_pandas(qa_df).remove_columns(['__index_level_0__'])\n",
    "qa_dataset_dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c7175",
   "metadata": {},
   "source": [
    "## 6. Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30817850-c205-424d-98bd-d46836282a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "        num_rows: 1509096\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "        num_rows: 15244\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db500a2babec4f9a86d28b40af9fb9b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/8 shards):   0%|          | 0/1509096 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1a721b741f48e391ee11ce1ebd6e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/15244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_dataset_with_retrieval_output_path = \"data/wikiqa/wikipedia-cn-20230720-dataset\"\n",
    "qa_dataset_train_test = qa_dataset_dedup.train_test_split(test_size=0.01, shuffle=True)\n",
    "print(qa_dataset_train_test)\n",
    "qa_dataset_train_test.save_to_disk(qa_dataset_with_retrieval_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "689c4d2f",
   "metadata": {},
   "source": [
    "## 7. evaluate retrieval on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c0adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"data/\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

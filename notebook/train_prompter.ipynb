{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6cb17e-af13-41b5-a69a-0effbb004160",
   "metadata": {},
   "source": [
    "## 1. init model\n",
    "### 1.1 config model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2d2e79-e694-4978-bd0c-cf8c6ae55e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a76da14f-1eb6-4a4b-b350-18b5daa51400",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from modeling_rankprompter2 import RankPrompter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prompter_tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n",
    "prompter_config = AutoConfig.from_pretrained(\"google/umt5-small\")\n",
    "# baichuan\n",
    "language_model_config = AutoConfig.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\n",
    "language_model_tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\n",
    "language_model_tokenizer.pad_token_id = language_model_config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f78b072-62ca-4500-9d65-c40ddf4058be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompter trainable params: 0.31B || all params: 0.31B || trainable%: 100.0000\n"
     ]
    }
   ],
   "source": [
    "from misc import count_parameters\n",
    "prompter_config.num_soft_prompt_tokens = 32\n",
    "prompter_config.llm_dim = language_model_config.hidden_size\n",
    "prompter = RankPrompter(prompter_config).to(device)\n",
    "trainable_params, all_param = count_parameters(prompter)\n",
    "print(\"prompter trainable params: {:.2f}B || all params: {:.2f}B || trainable%: {:.4f}\".format(\n",
    "        trainable_params / 1e9, all_param / 1e9, 100 * trainable_params / all_param\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c04e534-2b2e-4f72-822a-7a7bb899d52e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f8e40fd5f34d018e53b9f210411a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language model trainable params: 0.00B || all params: 7.00B || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# language_model_path = \"/data2/huggingface_models/baichuan-inc/Baichuan-7B\"\n",
    "language_model_path = \"/root/autodl-tmp/Baichuan-7B/\"\n",
    "language_model = AutoModelForCausalLM.from_pretrained(language_model_path, \n",
    "                device_map=device, trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "language_model.requires_grad_(False) # fix all model params\n",
    "trainable_params, all_param = count_parameters(language_model)\n",
    "print(\"language model trainable params: {:.2f}B || all params: {:.2f}B || trainable%: {:.4f}\".format(\n",
    "        trainable_params / 1e9, all_param / 1e9, 100 * trainable_params / all_param\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c24c2808-9aef-4025-9f99-d5d0a5b8a29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load prompter if needed\n",
    "# ckpt = torch.load(\"/root/autodl-tmp/saved_model/prompter/best_ckpt.pt\")\n",
    "# prompter.load_state_dict(ckpt[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "139ba7d0-36a6-425e-a135-41184b452bc2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RankPrompter' object has no attribute 'soft_prompt_embeds'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoft_prompt_embeds\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RankPrompter' object has no attribute 'soft_prompt_embeds'"
     ]
    }
   ],
   "source": [
    "prompter.soft_prompt_embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a88e1-5413-4cf4-90a0-8feea572085f",
   "metadata": {},
   "source": [
    "## 2. Init Dataset\n",
    "### 2.1 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b2379d-958d-4f19-9bd7-33a08da5b156",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "document_path = \"wikipedia-cn-20230720-documents_10k.json\"\n",
    "qa_path = \"wikipedia-cn-20230720_qa-with-retrieval_10k/\"\n",
    "\n",
    "docid2doc = {d[\"docid\"]:d[\"document\"] for d in json.load(open(document_path))}\n",
    "\n",
    "qa_dataset = load_from_disk(qa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdeb0b-25c4-484f-b259-1ae5c7695e5d",
   "metadata": {},
   "source": [
    "### 2.2 tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34fb47da-b921-46d4-ae31-51303b687b48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    # \n",
    "    num_doc = 20\n",
    "    doc_max_length = 512\n",
    "    ques_max_length = 32\n",
    "    ans_max_length = 128\n",
    "    #\n",
    "    pos_docid = example[\"docid\"]\n",
    "    # put pos_docid in the first place\n",
    "    docids = [pos_docid] + [docid for docid in example[\"retrieved_docids\"] if docid != pos_docid]\n",
    "    docs = [docid2doc[docid] for docid in docids[:num_doc]]\n",
    "    # padding to specific length, make all example have the same shape\n",
    "    prompter_tokenzied_docs = prompter_tokenizer(docs, padding=\"max_length\", \n",
    "                                                truncation=True, max_length=doc_max_length)\n",
    "    prompter_tokenzied_question = prompter_tokenizer(example[\"question\"], padding=\"max_length\", \n",
    "                                                truncation=True, max_length=ques_max_length)\n",
    "    prompter_tokenzied_answer = prompter_tokenizer(example[\"answer\"], padding=\"max_length\", \n",
    "                                                truncation=True, max_length=ans_max_length)\n",
    "    language_model_tokenzied_question = language_model_tokenizer(example[\"question\"], padding=\"max_length\",\n",
    "                                                truncation=True, max_length=ques_max_length)\n",
    "    language_model_tokenzied_answer = language_model_tokenizer(example[\"answer\"], padding=\"max_length\",\n",
    "                                                truncation=True, max_length=ans_max_length)\n",
    "    return {\"document_input_ids\": prompter_tokenzied_docs.input_ids,\n",
    "            \"document_attention_mask\": prompter_tokenzied_docs.attention_mask,\n",
    "            \"prompter_question_input_ids\": prompter_tokenzied_question.input_ids,\n",
    "            \"prompter_question_attention_mask\": prompter_tokenzied_question.attention_mask,\n",
    "            \"prompter_answer_input_ids\": prompter_tokenzied_answer.input_ids,\n",
    "            \"prompter_answer_attention_mask\": prompter_tokenzied_answer.attention_mask,\n",
    "            \"language_model_question_input_ids\": language_model_tokenzied_question.input_ids,\n",
    "            \"language_model_question_attention_mask\": language_model_tokenzied_question.attention_mask,\n",
    "            \"language_model_answer_input_ids\": language_model_tokenzied_answer.input_ids,\n",
    "            \"language_model_answer_attention_mask\": language_model_tokenzied_answer.attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "408250be-5bde-4acf-b80f-c315c91eea85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_qa_dataset = qa_dataset.map(preprocess_dataset, \n",
    "                                    num_proc=16, \n",
    "                                    remove_columns=[\"retrieved_docids\"]).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d0bf7d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'docid', 'document_input_ids', 'document_attention_mask', 'prompter_question_input_ids', 'prompter_question_attention_mask', 'prompter_answer_input_ids', 'prompter_answer_attention_mask', 'language_model_question_input_ids', 'language_model_question_attention_mask', 'language_model_answer_input_ids', 'language_model_answer_attention_mask'],\n",
       "        num_rows: 55192\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'docid', 'document_input_ids', 'document_attention_mask', 'prompter_question_input_ids', 'prompter_question_attention_mask', 'prompter_answer_input_ids', 'prompter_answer_attention_mask', 'language_model_question_input_ids', 'language_model_question_attention_mask', 'language_model_answer_input_ids', 'language_model_answer_attention_mask'],\n",
       "        num_rows: 6133\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82030a-a6cb-45cd-9ec0-df82688c439e",
   "metadata": {},
   "source": [
    "### 2.3 init dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c2b1977-a757-4d7d-a7f4-d9bfec96249b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# Create a DataLoader with the desired batch size\n",
    "batch_size = 1 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "gradient_accumulation_steps = 16 # accumulate gradients over n batches\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_qa_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_qa_dataset[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ba2c3",
   "metadata": {},
   "source": [
    "## 3. train\n",
    "### 3.1 config optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ddba016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "num_training_steps: 3449\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "# optimizer config \n",
    "learning_rate = 1e-4\n",
    "# Create AdamW optimizer and use the fused version if it is available\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device == 'cuda'\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "optimizer = torch.optim.AdamW(prompter.parameters(), lr=learning_rate, **extra_args)\n",
    "print(f\"using fused AdamW: {use_fused}\")\n",
    "# scheduler config\n",
    "num_epochs = 1\n",
    "num_training_steps = num_epochs * len(train_dataloader) // gradient_accumulation_steps\n",
    "lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "    optimizer=optimizer,  # scheduler是针对optimizer的lr的\n",
    "    lr_end=1e-7,\n",
    "    power=1, # 当power=1时（默认）等价于linear_schedule_with_warmup\n",
    "    num_warmup_steps=1000 // gradient_accumulation_steps,\n",
    "    num_training_steps=num_training_steps)\n",
    "print(f\"num_training_steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47941e25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just for check lr scheduler, which make scheduler empty, not run this cell when training\n",
    "# from matplotlib import pyplot as plt\n",
    "# lst = []\n",
    "# for _ in range(num_training_steps):\n",
    "#     lr_scheduler.step()\n",
    "#     lst.append(lr_scheduler.get_lr())\n",
    "# plt.plot(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998387b9",
   "metadata": {},
   "source": [
    "### 3.2 config evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77ce438c-fd4e-45a2-80bc-a92de42f8584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_language_model_labels(soft_prompts, language_model_question_input_ids, language_model_answer_input_ids):\n",
    "    prompt_labels = torch.zeros(\n",
    "            soft_prompts.shape[:2], dtype=torch.long, device=device\n",
    "        ).fill_(-100)\n",
    "    ques_labels = torch.zeros(\n",
    "            language_model_question_input_ids.shape[:2], dtype=torch.long, device=device\n",
    "        ).fill_(-100)\n",
    "    answer_labels = language_model_answer_input_ids.masked_fill(\n",
    "            language_model_answer_input_ids == language_model_tokenizer.pad_token_id, -100\n",
    "        ).to(device)\n",
    "    language_model_labels = torch.cat([prompt_labels, ques_labels, answer_labels], dim=1)\n",
    "    return language_model_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff77bc0e-3b5c-4421-9a48-7546f5b02b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def model_forward(batch):\n",
    "    document_input_ids = batch[\"document_input_ids\"].to(device)\n",
    "    document_attention_mask = batch[\"document_attention_mask\"].to(device)\n",
    "    prompter_question_input_ids = batch[\"prompter_question_input_ids\"].to(device)\n",
    "    prompter_question_attention_mask = batch[\"prompter_question_attention_mask\"].to(device)\n",
    "    prompter_output = prompter(\n",
    "        document_input_ids=document_input_ids,\n",
    "        document_attention_mask=document_attention_mask,\n",
    "        question_input_ids=prompter_question_input_ids,\n",
    "        question_attention_mask=prompter_question_attention_mask,\n",
    "    )\n",
    "    language_model_answer_input_ids = batch[\"language_model_answer_input_ids\"].to(device)\n",
    "    language_model_ans_embeds = language_model.get_input_embeddings()(language_model_answer_input_ids)\n",
    "    language_model_question_input_ids = batch[\"language_model_question_input_ids\"].to(device)\n",
    "    language_model_ques_embeds = language_model.get_input_embeddings()(language_model_question_input_ids)\n",
    "    language_model_input_embeds = torch.cat([prompter_output.soft_prompts, \n",
    "                                             language_model_ques_embeds, language_model_ans_embeds], dim=1)\n",
    "    language_model_labels = generate_language_model_labels(prompter_output.soft_prompts, \n",
    "                                                           language_model_question_input_ids,\n",
    "                                                           language_model_answer_input_ids)\n",
    "    language_model_output = language_model(inputs_embeds=language_model_input_embeds, \n",
    "                                           labels = language_model_labels)\n",
    "    return prompter_output, language_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7030e5e4-35cc-4540-928d-01c09d2bf29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import metric\n",
    "importlib.reload(metric)\n",
    "\n",
    "# helps estimate the loss of the model\n",
    "@torch.no_grad()\n",
    "def evaluate_prompter(prompter, language_model, dataloader, eval_iters=None):\n",
    "    out = {}\n",
    "    prompter.eval()\n",
    "    language_model.eval()\n",
    "    losses, rank_losses, lm_losses = [], [], []\n",
    "    generation_metrics = metric.GenerationMetrics(language_model_tokenizer)\n",
    "    retrieval_metrics = metric.RetrievalMetrics()\n",
    "    eval_iters = len(dataloader) if eval_iters is None else eval_iters\n",
    "    step = 0\n",
    "    for batch in tqdm(dataloader, desc=\"eval\", total=eval_iters):\n",
    "        prompter_output, language_model_output = model_forward(batch)\n",
    "        # loss\n",
    "        loss = (prompter_output.loss + language_model_output.loss).item()\n",
    "        losses.append(loss)\n",
    "        rank_losses.append(prompter_output.loss.item())\n",
    "        lm_losses.append(language_model_output.loss.item())\n",
    "        # generate metric\n",
    "        language_model_question_input_ids = batch[\"language_model_question_input_ids\"].to(device)\n",
    "        language_model_ques_embeds = language_model.get_input_embeddings()(language_model_question_input_ids)\n",
    "        language_model_input_embeds = torch.cat([prompter_output.soft_prompts, \n",
    "                                                 language_model_ques_embeds], dim=1)\n",
    "        language_model_pred = language_model.generate(inputs_embeds=language_model_input_embeds, max_new_tokens=64)\n",
    "        generation_metrics.update(language_model_pred.cpu(), batch[\"language_model_answer_input_ids\"])\n",
    "        # rank metric\n",
    "        batch_size, num_doc = batch[\"document_input_ids\"].shape[:2]\n",
    "        rank_preds = prompter_output.logits.cpu().tolist()\n",
    "        rank_targets = [[True] + [False] * (num_doc - 1) for _ in range(batch_size)]\n",
    "        retrieval_metrics.update(rank_preds, rank_targets)\n",
    "        # \n",
    "        if step > eval_iters:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    out[\"loss\"] = {\"val_loss\": np.mean(losses), \"val_rank_loss\": np.mean(rank_losses),\n",
    "                   \"val_lm_loss\": np.mean(lm_losses)}\n",
    "    out[\"generation\"] = generation_metrics.compute()\n",
    "    out[\"retrieval\"] = retrieval_metrics.compute()\n",
    "    prompter.train()\n",
    "    language_model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f41ca889-b3bc-46f8-af6a-4e6a35baa482",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_output_dir = Path(\"/root/autodl-tmp/saved_model/prompter\")\n",
    "model_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "log_interval = 1000\n",
    "eval_iters = 500\n",
    "total_micro_steps = num_training_steps * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:42: UserWarning: The ``compute`` method of metric MeanMetric was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro step 0/55184:  loss 4.5071  rank-loss 0.7571  lm-loss 3.7500  lr 0.0000000\n",
      "micro step 1000/55184:  loss 2.6397  rank-loss 0.5537  lm-loss 2.0859  lr 0.0001000\n",
      "micro step 2000/55184:  loss 1.9683  rank-loss 0.2091  lm-loss 1.7592  lr 0.0000981\n",
      "micro step 3000/55184:  loss 1.8656  rank-loss 0.2013  lm-loss 1.6642  lr 0.0000963\n",
      "micro step 4000/55184:  loss 1.8976  rank-loss 0.1988  lm-loss 1.6988  lr 0.0000945\n",
      "micro step 5000/55184:  loss 1.8869  rank-loss 0.1978  lm-loss 1.6891  lr 0.0000926\n",
      "micro step 6000/55184:  loss 1.8673  rank-loss 0.1985  lm-loss 1.6688  lr 0.0000908\n",
      "micro step 7000/55184:  loss 1.8672  rank-loss 0.1970  lm-loss 1.6701  lr 0.0000889\n",
      "micro step 8000/55184:  loss 1.8177  rank-loss 0.1979  lm-loss 1.6198  lr 0.0000871\n",
      "micro step 9000/55184:  loss 1.8108  rank-loss 0.1966  lm-loss 1.6142  lr 0.0000853\n",
      "micro step 10000/55184:  loss 1.8606  rank-loss 0.1969  lm-loss 1.6637  lr 0.0000834\n",
      "micro step 11000/55184:  loss 1.8072  rank-loss 0.1964  lm-loss 1.6108  lr 0.0000816\n",
      "micro step 12000/55184:  loss 1.8475  rank-loss 0.1967  lm-loss 1.6508  lr 0.0000797\n",
      "micro step 13000/55184:  loss 1.7862  rank-loss 0.1952  lm-loss 1.5910  lr 0.0000779\n",
      "micro step 14000/55184:  loss 1.8181  rank-loss 0.1959  lm-loss 1.6222  lr 0.0000760\n",
      "micro step 15000/55184:  loss 1.8374  rank-loss 0.1962  lm-loss 1.6412  lr 0.0000742\n",
      "micro step 16000/55184:  loss 1.7864  rank-loss 0.1949  lm-loss 1.5915  lr 0.0000723\n",
      "micro step 17000/55184:  loss 1.8716  rank-loss 0.1949  lm-loss 1.6767  lr 0.0000705\n",
      "micro step 18000/55184:  loss 1.8445  rank-loss 0.1954  lm-loss 1.6490  lr 0.0000686\n",
      "micro step 19000/55184:  loss 1.8655  rank-loss 0.1950  lm-loss 1.6705  lr 0.0000668\n",
      "micro step 20000/55184:  loss 1.7723  rank-loss 0.1943  lm-loss 1.5780  lr 0.0000650\n",
      "micro step 21000/55184:  loss 1.8529  rank-loss 0.1932  lm-loss 1.6596  lr 0.0000631\n",
      "micro step 22000/55184:  loss 1.8794  rank-loss 0.1935  lm-loss 1.6859  lr 0.0000613\n",
      "micro step 23000/55184:  loss 1.8744  rank-loss 0.1967  lm-loss 1.6777  lr 0.0000594\n",
      "micro step 24000/55184:  loss 1.8429  rank-loss 0.1970  lm-loss 1.6459  lr 0.0000576\n",
      "micro step 25000/55184:  loss 1.8251  rank-loss 0.1964  lm-loss 1.6287  lr 0.0000558\n",
      "micro step 26000/55184:  loss 1.8338  rank-loss 0.1959  lm-loss 1.6379  lr 0.0000539\n",
      "micro step 27000/55184:  loss 1.8136  rank-loss 0.1951  lm-loss 1.6184  lr 0.0000521\n",
      "micro step 28000/55184:  loss 1.8870  rank-loss 0.1951  lm-loss 1.6919  lr 0.0000502\n",
      "micro step 29000/55184:  loss 1.7630  rank-loss 0.1935  lm-loss 1.5695  lr 0.0000484\n",
      "micro step 30000/55184:  loss 1.8282  rank-loss 0.1939  lm-loss 1.6343  lr 0.0000465\n",
      "micro step 31000/55184:  loss 1.8138  rank-loss 0.1933  lm-loss 1.6206  lr 0.0000447\n",
      "micro step 32000/55184:  loss 1.8031  rank-loss 0.1939  lm-loss 1.6093  lr 0.0000428\n",
      "micro step 33000/55184:  loss 1.8089  rank-loss 0.1936  lm-loss 1.6154  lr 0.0000410\n",
      "micro step 34000/55184:  loss 1.8160  rank-loss 0.1937  lm-loss 1.6223  lr 0.0000392\n",
      "micro step 35000/55184:  loss 1.8328  rank-loss 0.1921  lm-loss 1.6406  lr 0.0000373\n",
      "micro step 36000/55184:  loss 1.8507  rank-loss 0.1930  lm-loss 1.6577  lr 0.0000355\n",
      "micro step 37000/55184:  loss 1.8764  rank-loss 0.1921  lm-loss 1.6843  lr 0.0000336\n",
      "micro step 38000/55184:  loss 1.7639  rank-loss 0.1916  lm-loss 1.5723  lr 0.0000318\n",
      "micro step 39000/55184:  loss 1.7838  rank-loss 0.1923  lm-loss 1.5914  lr 0.0000299\n",
      "micro step 40000/55184:  loss 1.8090  rank-loss 0.1914  lm-loss 1.6176  lr 0.0000281\n",
      "micro step 41000/55184:  loss 1.8369  rank-loss 0.1906  lm-loss 1.6463  lr 0.0000263\n",
      "micro step 42000/55184:  loss 1.8210  rank-loss 0.1921  lm-loss 1.6289  lr 0.0000244\n",
      "micro step 43000/55184:  loss 1.8018  rank-loss 0.1922  lm-loss 1.6096  lr 0.0000226\n",
      "micro step 44000/55184:  loss 1.7974  rank-loss 0.1906  lm-loss 1.6068  lr 0.0000207\n",
      "micro step 45000/55184:  loss 1.7783  rank-loss 0.1902  lm-loss 1.5881  lr 0.0000189\n",
      "micro step 46000/55184:  loss 1.8215  rank-loss 0.1917  lm-loss 1.6299  lr 0.0000170\n",
      "micro step 47000/55184:  loss 1.7915  rank-loss 0.1912  lm-loss 1.6002  lr 0.0000152\n",
      "micro step 48000/55184:  loss 1.7895  rank-loss 0.1914  lm-loss 1.5980  lr 0.0000133\n",
      "micro step 49000/55184:  loss 1.8088  rank-loss 0.1900  lm-loss 1.6188  lr 0.0000115\n",
      "micro step 50000/55184:  loss 1.8594  rank-loss 0.1905  lm-loss 1.6689  lr 0.0000097\n",
      "micro step 51000/55184:  loss 1.8151  rank-loss 0.1900  lm-loss 1.6250  lr 0.0000078\n",
      "micro step 52000/55184:  loss 1.7892  rank-loss 0.1893  lm-loss 1.5999  lr 0.0000060\n",
      "micro step 53000/55184:  loss 1.8049  rank-loss 0.1899  lm-loss 1.6149  lr 0.0000041\n",
      "micro step 54000/55184:  loss 1.8462  rank-loss 0.1881  lm-loss 1.6580  lr 0.0000023\n",
      "micro step 55000/55184:  loss 1.8068  rank-loss 0.1899  lm-loss 1.6169  lr 0.0000005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:  72%|███████▏  | 358/500 [10:46<04:30,  1.90s/it]"
     ]
    }
   ],
   "source": [
    "from torchmetrics.aggregation import RunningMean\n",
    "train_loss = RunningMean(window=log_interval).to(device)\n",
    "train_loss_rank = RunningMean(window=log_interval).to(device)\n",
    "train_loss_lm = RunningMean(window=log_interval).to(device)\n",
    "\n",
    "step = 0 # total steps = num_training_steps * gradient_accumulation_steps\n",
    "best_val_loss = 1e9\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        prompter_output, language_model_output = model_forward(batch)                                  \n",
    "        loss = (prompter_output.loss + language_model_output.loss) / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        train_loss_rank.update(prompter_output.loss)\n",
    "        train_loss_lm.update(language_model_output.loss)\n",
    "        train_loss.update(loss * gradient_accumulation_steps)\n",
    "        # log the loss on train set\n",
    "        if step % log_interval == 0:\n",
    "            print(f\"micro step {step}/{total_micro_steps}:\",\n",
    "                  f\"loss {train_loss.compute():.4f}\",\n",
    "                  f\"rank-loss {train_loss_rank.compute():.4f}\",\n",
    "                  f\"lm-loss {train_loss_lm.compute():.4f}\",\n",
    "                  f\"lr {lr_scheduler.get_lr()[0]:.7f}\", sep=\"  \")\n",
    "        step += 1\n",
    "        \n",
    "    eval_results = evaluate_prompter(prompter, language_model, test_dataloader, \n",
    "                                     eval_iters=eval_iters)\n",
    "    if eval_results['loss']['val_loss'] < best_val_loss:\n",
    "        best_val_loss = eval_results['loss']['val_loss']\n",
    "        checkpoint = {\n",
    "            'model': prompter.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': prompter_config,\n",
    "            'iter_num': step,\n",
    "            'eval_results': eval_results,\n",
    "            'config': None,\n",
    "        }\n",
    "        print(f\"saving checkpoint to {model_output_dir}\")\n",
    "        torch.save(checkpoint, model_output_dir / 'best_ckpt.pt')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4572b4ea-dd71-4ed2-92fd-a57ca4b58129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: ['Marvin Friedrich曾经效力于哪个德甲球队？']\n",
      "a: ['Marvin Friedrich曾经效力于史浩克04这个德甲球队。']\n",
      "prompter a: ['Marvin Friedrich曾经效力于德甲球队科隆。他在2009年1月从科隆转会到德甲球队汉堡。他在2010年1月从汉堡转会到德甲球队美因茨。他在2010年12月从美因茨转会到德甲球队科隆。他在2011年1月从科隆转会到德甲球队汉诺威96。他在2011年12月从汉诺威96转会到德甲球队云达不莱梅。他在']\n",
      "origin a: ['\\n拜仁慕尼黑\\n\\n\\n\\n 展开全部 拜仁慕尼黑\\n\\n\\n\\n 展开全部 拜仁慕尼黑\\n\\n\\n\\n 展开全部 拜仁慕尼黑\\n\\n\\n\\n 展开全部 拜仁慕尼黑\\n\\n\\n\\n 展开全部 拜仁慕尼黑']\n",
      "\n",
      "q: ['有哪些经典的神学论战案例？']\n",
      "a: ['马丁·路德在1525年的著作《意志的束缚》，是神学论战的一个典型例子。另一个例子是两册1896年的著作《基督教世界科学与神学论战史》（分上、下卷），由美国外交家，教育家及作家安德鲁·迪克森·怀特所著。']\n",
      "prompter a: ['经典的神学论战案例包括：亚伯拉罕论战、摩西论战、耶稣论战、保罗论战、奥古斯丁论战、阿奎那论战、加尔文论战、马丁·路德论战、加尔文论战、清教徒论战、加尔文论战、马丁·路德论战、加尔文论战、清教徒论战、加尔文论战、马丁·路德论战、加尔文论战、清教徒论战、加尔文论战']\n",
      "origin a: ['噢，我明白了，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是想说，你就是']\n",
      "\n",
      "q: ['西安路有哪些社会事业？']\n",
      "a: ['西安路有大连市口腔医院、大连机车医院、大连市第八中学、大连市朝鲜族基督教堂等社会事业。']\n",
      "prompter a: ['西安路有西安路小学、西安路中学、西安路幼儿园、西安路社区卫生服务中心等社会事业。西安路小学是西安路街道辖区内的一所公办小学，西安路中学是西安路街道辖区内的一所公办中学。西安路幼儿园是西安路街道辖区内的一所公办幼儿园。西安路社区卫生服务中心是西安路街道辖区内的一所社区卫生服务中心。西安路社区卫生服务中心是西安路街道辖区内的一所社区卫生服务中心。西安路社区卫生服务中心是西安路街道辖区内的一所社区卫生服务中心。西安路社区卫生服务中心是西安路街道辖区']\n",
      "origin a: ['']\n",
      "\n",
      "q: ['乌翅真鲨的生殖方式是什么？']\n",
      "a: ['乌翅真鲨是胎生的，每胎约有2至4头小鲨。母鲨的怀胎期为16个月。出生的小鲨约为33至52厘米长。']\n",
      "prompter a: ['乌翅真鲨的生殖方式是卵胎生。雌性乌翅真鲨在产卵后会将卵产在雄性乌翅真鲨的育儿袋中，由雄性乌翅真鲨负责孵化。孵化后的幼鲨会由雄性乌翅真鲨负责喂养。幼鲨在出生后约10个月左右会离开育儿袋，并开始独立生活。幼鲨在离开育儿袋后，会由雌性乌翅真鲨负责喂养。幼鲨在离开育儿袋后，会由雌性乌翅真鲨']\n",
      "origin a: ['\\n乌翅真鲨的生殖方式是什么？  乌翅真鲨的生殖方式是卵胎生。  乌翅真鲨(学名：Carcharhinus limbatus)，又称黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、黑真鲨、']\n",
      "\n",
      "q: ['What awards has Liu Xinglong won?']\n",
      "a: ['Liu Xinglong has won many awards, including the Red Chamber Dream Award, the Chinese Contemporary Literature Academy Award, the Chinese Novel Society Long Novel Award, the Mao Dun Literature Award, the Lu Xun Literature Award, and the Youth Literature Creation Achievement Award.']\n",
      "prompter a: [\"Liu Xinglong has won the 2008 Chinese Golden Eagle Award for Best Actor.\\nWhat is the plot of the movie?\\nThe plot of the movie is about a young man who is a member of the Chinese Communist Party and is sent to a remote village to investigate a murder.\\nWhat is the movie's rating?\\nThe movie has a rating of 7.8/10 on IMDb.\\nWhat is the movie's runtime?\\nThe movie has a runtime of 100 minutes.\\nWhat is the movie's genre?\\nThe movie is a\"]\n",
      "origin a: ['\\nLiu Xinglong has won 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu Xinglong has been nominated for 10 awards in the past.\\nLiu X']\n",
      "\n",
      "q: ['冕牌玻璃与其他光学玻璃有什么相似之处？']\n",
      "a: ['除了具体以材料命名的冕牌玻璃之外，其他的光学玻璃也有与被称为冕牌玻璃性质相似的产品。通常，只要阿贝数在50－85之间的玻璃，像是肖特硼矽酸盐玻璃：BK7，是一种很常见，用在精密透镜上的冕牌玻璃。']\n",
      "prompter a: ['冕牌玻璃与其他光学玻璃相似之处在于它们都具有良好的透光性。冕牌玻璃的透光率比其他光学玻璃高，但比冕牌玻璃低。冕牌玻璃的透光率比冕牌玻璃低，但比冕牌玻璃高。冕牌玻璃的透光率比冕牌玻璃高，但比冕牌玻璃低。冕牌玻璃的透光率比冕牌玻璃低，但比冕牌玻璃高。冕牌玻璃的透光率比冕牌玻璃高，但比冕牌玻璃低。冕牌玻璃的透光率']\n",
      "origin a: ['\\n冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似之处？ 冕牌玻璃与其他光学玻璃有什么相似']\n",
      "\n",
      "q: ['金子美铃的全集是由谁翻译的？']\n",
      "a: ['金子美铃的全集是由田原翻译的，由中国雅众文化出品，中信出版社出版。']\n",
      "prompter a: ['金子美铃的全集是由日本作家、翻译家林真理子翻译的。林真理子在2012年出版了《金子美铃诗集》，并于2013年出版了《金子美铃全集》。林真理子在2013年10月10日于东京都内举办的“金子美铃全集”新书发表会中，宣布将金子美铃的全集翻译成中文。林真理子在2014年1月1日于台湾出版了《金']\n",
      "origin a: ['\\n金子美铃的全集是由谁翻译的？了解更多金子美铃的全集，请关注微信公众号：金子美铃的全集']\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in test_dataloader:    \n",
    "    print(\"q:\", batch[\"question\"])\n",
    "    print(\"a:\", batch[\"answer\"])\n",
    "    language_model_question_input_ids = batch[\"language_model_question_input_ids\"].to(device)\n",
    "    language_model_ques_embeds = language_model.get_input_embeddings()(language_model_question_input_ids)\n",
    "    language_model_input_embeds = torch.cat([prompter_output.soft_prompts, \n",
    "                                             language_model_ques_embeds], dim=1)\n",
    "    pred = language_model.generate(inputs_embeds=language_model_input_embeds, max_new_tokens=128)\n",
    "    origin_pred = language_model.generate(inputs_embeds=language_model_ques_embeds, max_new_tokens=128)\n",
    "    print(\"prompter a:\", language_model_tokenizer.batch_decode(pred.cpu(), skip_special_tokens=True))\n",
    "    print(\"origin a:\", language_model_tokenizer.batch_decode(origin_pred.cpu(), skip_special_tokens=True))\n",
    "    if i > 5:\n",
    "        break\n",
    "    i += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262fef0-e40b-40f1-af00-e1c7a15d4416",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03b596-66b2-4cf6-8900-1061fe3a9617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch[\"document_input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e30d82e-c134-4a01-b442-a2c8176de11a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_model_output = language_model(inputs_embeds=language_model_input_embeds, \n",
    "                                       labels = language_model_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674ec24-c358-403b-921b-388934b21911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language_model_tokenizer.batch_decode(torch.argmax(language_model_output.logits, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a498163-5080-43a7-bb8c-58f3d02472d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = prompter_tokenizer.batch_decode(batch[\"document_input_ids\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246c516-6946-4e06-966c-b844bda35d02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompter_output.logits.topk(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e19d4d-fd1a-42fb-b47d-2fb16f33d313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c1e0a-49e9-4e0a-9659-e6db224a3239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0dc3e-51af-4699-bbff-e0d68c7b1ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import metric\n",
    "importlib.reload(metric)\n",
    "generation_metrics = metric.GenerationMetrics(language_model_tokenizer)\n",
    "generation_metrics.update(pred.cpu(), batch[\"language_model_answer_input_ids\"])\n",
    "generation_metrics_scores = generation_metrics.compute()\n",
    "generation_metrics_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac9d51-0893-423c-a567-2f4c7ef7e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluate_prompter(prompter, language_model, test_dataloader, eval_iters=100)\n",
    "print(json.dumps(eval_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cff7a286-9a8b-455b-9f3c-ce21cdfb6cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 101it [03:00,  1.78s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"loss\": {\n",
      "    \"val_loss\": 1.790435386931195,\n",
      "    \"val_rank_loss\": 0.19559393574794134,\n",
      "    \"val_lm_loss\": 1.5948414522058822\n",
      "  },\n",
      "  \"generation\": {\n",
      "    \"accuracy\": 0.0,\n",
      "    \"rouge-1\": 40.217145098039204,\n",
      "    \"rouge-2\": 20.987956862745097,\n",
      "    \"rouge-l\": 28.878924509803916,\n",
      "    \"bleu-4\": 14.73178823529412\n",
      "  },\n",
      "  \"retrieval\": {\n",
      "    \"HitRate@1\": 0.11764705926179886,\n",
      "    \"HitRate@5\": 0.36274510622024536,\n",
      "    \"HitRate@10\": 0.6666666865348816,\n",
      "    \"MRR\": 0.27186647057533264,\n",
      "    \"MAP@1\": 0.11764705926179886,\n",
      "    \"MAP@5\": 0.20718954503536224,\n",
      "    \"MAP@10\": 0.2503190338611603,\n",
      "    \"NDCG@1\": 0.11764705926179886,\n",
      "    \"NDCG@5\": 0.24587662518024445,\n",
      "    \"NDCG@10\": 0.3467034697532654,\n",
      "    \"Recall@1\": 0.11764705926179886,\n",
      "    \"Recall@5\": 0.36274510622024536,\n",
      "    \"Recall@10\": 0.6666666865348816\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 训练1轮，每次argmax取出一个soft prompt作为输入，\n",
    "\n",
    "eval_results = evaluate_prompter(prompter, language_model, test_dataloader, eval_iters=100)\n",
    "print(json.dumps(eval_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "659f0ea8-63df-45b9-82e8-fd531a26726c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': {'val_loss': 1.7964331930979769,\n",
       "  'val_rank_loss': 0.1924001278193841,\n",
       "  'val_lm_loss': 1.6040330653903574},\n",
       " 'generation': {'accuracy': 0.0,\n",
       "  'rouge-1': 9.436148573292026,\n",
       "  'rouge-2': 0.10847696070438612,\n",
       "  'rouge-l': 6.463228501548997,\n",
       "  'bleu-4': 0.6537248165661177},\n",
       " 'retrieval': {'HitRate@1': 0.1159302145242691,\n",
       "  'HitRate@5': 0.438284695148468,\n",
       "  'HitRate@10': 0.6991684436798096,\n",
       "  'MRR': 0.28068479895591736,\n",
       "  'MAP@1': 0.1159302145242691,\n",
       "  'MAP@5': 0.22521333396434784,\n",
       "  'MAP@10': 0.25961822271347046,\n",
       "  'NDCG@1': 0.1159302145242691,\n",
       "  'NDCG@5': 0.277713805437088,\n",
       "  'NDCG@10': 0.3616619110107422,\n",
       "  'Recall@1': 0.1159302145242691,\n",
       "  'Recall@5': 0.438284695148468,\n",
       "  'Recall@10': 0.6991684436798096}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdad7240-5a33-458a-8d69-ae7d10b14435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': {'val_loss': 2.8331266724517135,\n",
       "  'val_rank_loss': 0.19326534395080475,\n",
       "  'val_lm_loss': 2.639861329080385},\n",
       " 'generation': {'accuracy': 0.0,\n",
       "  'rouge-1': 14.224255013859448,\n",
       "  'rouge-2': 0.9631314691015815,\n",
       "  'rouge-l': 9.532860215229089,\n",
       "  'bleu-4': 1.5044573292026742},\n",
       " 'retrieval': {'HitRate@1': 0.10092940181493759,\n",
       "  'HitRate@5': 0.4130115807056427,\n",
       "  'HitRate@10': 0.6972118020057678,\n",
       "  'MRR': 0.2618274390697479,\n",
       "  'MAP@1': 0.10092940181493759,\n",
       "  'MAP@5': 0.20314691960811615,\n",
       "  'MAP@10': 0.24042972922325134,\n",
       "  'NDCG@1': 0.10092940181493759,\n",
       "  'NDCG@5': 0.25463661551475525,\n",
       "  'NDCG@10': 0.34589165449142456,\n",
       "  'Recall@1': 0.10092940181493759,\n",
       "  'Recall@5': 0.4130115807056427,\n",
       "  'Recall@10': 0.6972118020057678}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last eval\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60359d3d-07bf-4605-a5bc-8d39826c15b5",
   "metadata": {},
   "source": [
    "# last train loss\n",
    "\n",
    "micro step 50000/55184:  loss 1.7886  rank-loss 0.1931  lm-loss 1.5955  lr 0.0000097\n",
    "micro step 51000/55184:  loss 1.8216  rank-loss 0.1930  lm-loss 1.6286  lr 0.0000078\n",
    "micro step 52000/55184:  loss 1.7938  rank-loss 0.1927  lm-loss 1.6011  lr 0.0000060\n",
    "micro step 53000/55184:  loss 1.7911  rank-loss 0.1918  lm-loss 1.5993  lr 0.0000041\n",
    "micro step 54000/55184:  loss 1.8032  rank-loss 0.1921  lm-loss 1.6111  lr 0.0000023\n",
    "micro step 55000/55184:  loss 1.8028  rank-loss 0.1927  lm-loss 1.6101  lr 0.0000005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22825f-514a-44fd-a814-ec6930bc8b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13c52c9-d093-4800-ad05-efcf9cf48967",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58709ad-e2cd-4cb4-88f9-72b991aa2126",
   "metadata": {},
   "source": [
    "### 1.1 Install package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89ac1c-bf6c-48aa-843e-5ec2567bab5a",
   "metadata": {},
   "source": [
    "### 1.2 Setup Retriever\n",
    "here we use m3e-base as the chinese Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8e170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch1.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: * Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one\n",
      "Embedding: (768,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('moka-ai/m3e-base')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = [\n",
    "    '* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem',\n",
    "    '* Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练',\n",
    "    '* Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one'\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2db9c1-8648-4e1e-abda-c96d114beccf",
   "metadata": {},
   "source": [
    "## 2. Preprocess QA data\n",
    "### 2.1 clean qa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3637f787-7d1e-492e-a092-5317df844186",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "T2Ranking_dir = Path(\"data/T2Ranking\")\n",
    "T2Ranking_dir.mkdir(exist_ok=True)\n",
    "qa_input_path = T2Ranking_dir / \"dev_qa_pairs.json\"\n",
    "document_path = T2Ranking_dir / \"dev_document.json\"\n",
    "qa_data = json.load(open(qa_input_path))\n",
    "documents = json.load(open(document_path))\n",
    "print(\"qa pairs\", len(qa_data), \"documents\", len(documents))\n",
    "\n",
    "# 1. check documents\n",
    "docid2doc = {doc[\"docid\"]: doc[\"document\"] for doc in documents}\n",
    "print(\"docid2doc len:\", len(docid2doc))\n",
    "\n",
    "# 2. statistics and filter qa\n",
    "print(\"qa pairs num:\", len(qa_data))\n",
    "ques_lens = [len(qa[\"question\"]) for qa in qa_data]\n",
    "min_ques_len = min(ques_lens)\n",
    "max_ques_len = max(ques_lens)\n",
    "mean_ques_len = np.mean(ques_lens)\n",
    "print(\"max ques len:\", max_ques_len)\n",
    "print(\"min ques len:\", min_ques_len)\n",
    "print(\"mean ques len:\", mean_ques_len)\n",
    "\n",
    "# 3 filter doc\n",
    "doc_lens = [len(docid2doc[docid]) for docid in docid2doc]\n",
    "min_doc_len = min(doc_lens)\n",
    "max_doc_len = max(doc_lens)\n",
    "mean_doc_len = np.mean(doc_lens)\n",
    "print(\"max doc len:\", max_doc_len)\n",
    "print(\"min doc len:\", min_doc_len)\n",
    "print(\"mean doc len:\", mean_doc_len)\n",
    "\n",
    "print(\"before filter, doc num:\", len(docid2doc))\n",
    "documents_keep = []\n",
    "documents_drop = []\n",
    "ans_len_99 = np.percentile(doc_lens, 99)\n",
    "ans_len_1 = np.percentile(doc_lens, 1)\n",
    "print(\"ans_len_99:\", ans_len_99)\n",
    "print(\"ans_len_1:\", ans_len_1)\n",
    "for item in documents:\n",
    "    if (len(item[\"document\"]) <= ans_len_99 and \n",
    "        len(item[\"document\"]) >= ans_len_1):\n",
    "        documents_keep.append(item)\n",
    "    else:\n",
    "        documents_drop.append(item)\n",
    "print(\"after filter, doc num:\", len(documents_keep))\n",
    "print(\"drop ratio:\", len(documents_drop) / len(documents))\n",
    "\n",
    "docid2doc = {doc[\"docid\"]: doc[\"document\"] for doc in documents_keep}\n",
    "\n",
    "# 4. filter qa\n",
    "print(\"before filter, qa num:\", len(qa_data))\n",
    "qa_data_keep = []\n",
    "for item in qa_data:\n",
    "    if item[\"docid\"] in docid2doc:\n",
    "        qa_data_keep.append(item)\n",
    "print(\"after filter, qa num:\", len(qa_data_keep))\n",
    "\n",
    "# ques2docids\n",
    "ques2docids = defaultdict(list)\n",
    "for qa in qa_data_keep:\n",
    "    ques2docids[qa[\"question\"]].append(qa[\"docid\"])\n",
    "print(\"ques2docids len:\", len(ques2docids))\n",
    "\n",
    "# 5. save\n",
    "out_dir = Path(\"data/T2Ranking/\")\n",
    "qa_output_path = out_dir / \"dev_qa_pairs_filter.json\"\n",
    "document_output_path = out_dir / \"dev_document_filter.json\"\n",
    "json.dump(qa_data_keep, open(qa_output_path, \"w\"), indent=2, ensure_ascii=False)\n",
    "json.dump(documents_keep, open(document_output_path, \"w\"), indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09605b00-4269-45d4-986a-6d050a77fdbe",
   "metadata": {},
   "source": [
    "### 2.2 build faiss index for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5382346d-a200-4a51-bcf7-d6fd13752560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'document'],\n",
      "    num_rows: 2303643\n",
      "})\n",
      "{\n",
      "  \"docid\": \"3076cc10eea8\",\n",
      "  \"document\": \"找寄件人改号码\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# document_dataset = load_dataset(\"json\", data_files=document_path)\n",
    "document_dataset = Dataset.from_list(documents)\n",
    "print(document_dataset)\n",
    "print(json.dumps(document_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# if doc embedding has been processed, then load here\n",
    "document_dataset.load_faiss_index('doc_embedding', '/ssd4T/T2Ranking/doc_embedding.faiss')\n",
    "document_dataset_with_emb = document_dataset\n",
    "print(document_dataset_with_emb.is_index_initialized(\"doc_embedding\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feeaa53-2db3-4f80-9adf-506e9736a072",
   "metadata": {},
   "source": [
    "### 2.3 encode questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "809787f1-d6dc-4cbe-b730-311c66f23bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid'],\n",
      "    num_rows: 740960\n",
      "})\n",
      "{'question': '鹦鹉吃自己的小鱼吗', 'answer': '', 'docid': '3fef708246b3'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51fa6afae974567896de3f5cfdb5052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/740960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_dataset = Dataset.from_list(qa_data_keep)\n",
    "print(qa_dataset)\n",
    "print(qa_dataset[0])\n",
    "# \n",
    "qa_dataset_with_emb = qa_dataset.map(\n",
    "    lambda example: {'question_embedding': model.encode(example[\"question\"])}, \n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f82c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question'],\n",
      "    num_rows: 199576\n",
      "})\n",
      "{'question': '鹦鹉吃自己的小鱼吗'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caabd0d40d6a4ec4bfacf63526ff483d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/199576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_dataset = Dataset.from_list([{\"question\":q} for q in ques2docids])\n",
    "print(q_dataset)\n",
    "print(q_dataset[0])\n",
    "# \n",
    "q_dataset_with_emb = q_dataset.map(\n",
    "    lambda example: {'question_embedding': model.encode(example[\"question\"])}, \n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50022e28-58cf-4fa8-9052-39454a42e206",
   "metadata": {},
   "source": [
    "## 3. Build dataset\n",
    "### 3.1 retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "879e4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_topk_documents(example):\n",
    "    topk = 100\n",
    "    ques_embedding = np.array(example[\"question_embedding\"], dtype=np.float32)\n",
    "    scores, retrieved_examples = document_dataset_with_emb.get_nearest_examples('doc_embedding', ques_embedding, k=topk)\n",
    "    example[\"retrieved_docids\"] = retrieved_examples[\"docid\"]\n",
    "    example[\"retrieved_doc_scores\"] = scores.tolist()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec3faa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b527487e52429a85d8512bf58f29bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/199576 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据硬件，调整num_proc\n",
    "q_dataset_with_retrieval = q_dataset_with_emb.map(retrieve_topk_documents, \n",
    "                                                  num_proc=16,\n",
    "                                                  remove_columns=[\"question_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "238b3533",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2retrieval = {}\n",
    "for item in q_dataset_with_retrieval:\n",
    "    ques = item['question']\n",
    "    q2retrieval[ques] = dict(retrieved_docids=item['retrieved_docids'],\n",
    "                             retrieved_doc_scores=item['retrieved_doc_scores'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7d157ff-75e0-48b8-b636-71a681430c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 因为是一对多，这里为了训练调整成一对一\n",
    "def prepare_for_training_dataset(example):\n",
    "    ques = example[\"question\"]\n",
    "    ques_docids = ques2docids[ques]\n",
    "    ques_docids = [d for d in ques_docids if d != example[\"docid\"]]\n",
    "    retrieved_docids = q2retrieval[ques][\"retrieved_docids\"]\n",
    "    retrieved_doc_scores = q2retrieval[ques][\"retrieved_doc_scores\"]\n",
    "    doc_scores = []\n",
    "    docids = []\n",
    "    for d, s in zip(retrieved_docids, retrieved_doc_scores): \n",
    "        if d in docid2doc and d not in ques_docids:\n",
    "            doc_scores.append(s)\n",
    "            docids.append(d)\n",
    "    example[\"retrieved_docids\"] = docids\n",
    "    example[\"retrieved_doc_scores\"] = doc_scores\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "77ebd94a-8301-4fb2-bbd7-4144ef4b280b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8342f22f654851b2c6b7483da7029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/740960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据硬件，调整num_proc\n",
    "qa_dataset_with_retrieval = qa_dataset_with_emb.map(prepare_for_training_dataset, \n",
    "                                                    num_proc=1,\n",
    "                                                    remove_columns=[\"question_embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "596df340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "87\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(qa_dataset_with_retrieval[0][\"retrieved_docids\"]))\n",
    "print(len(qa_dataset_with_retrieval[0][\"retrieved_doc_scores\"]))\n",
    "print(qa_dataset_with_retrieval[3][\"docid\"] in qa_dataset_with_retrieval[3][\"retrieved_docids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc74bd4-0869-4ecb-89de-1997acc308b0",
   "metadata": {},
   "source": [
    "## 4. Evaluate\n",
    "### 4.1 Evaluate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b87f3f3b-bff8-4ef7-abe0-d1785b585666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    for pred, true_label in zip(predictions, true_labels):\n",
    "        if true_label in pred[:topk]:\n",
    "            num_correct += 1\n",
    "    \n",
    "    top1_accuracy = num_correct / len(predictions)\n",
    "    return top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "525e4540-8e9a-41b9-af90-47a2e89c7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 740960/740960 [02:11<00:00, 5623.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "questions, predictions, targets = [], [], []\n",
    "for s in tqdm(qa_dataset_with_retrieval):\n",
    "    questions.append(s[\"question\"])\n",
    "    predictions.append(s[\"retrieved_docids\"])\n",
    "    targets.append(s[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45558dc7-44fc-42df-a6f6-cc9c0b3b118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1: 0.1833931656229756\n",
      "top3: 0.3276033793996977\n",
      "top5: 0.39888388037141004\n",
      "top10: 0.4942102137767221\n",
      "top20: 0.5847265709350032\n",
      "top50: 0.6923882530770892\n",
      "top100: 0.7542728352407687\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "    topk_acc = compute_topk_accuracy(predictions, targets, topk=k)\n",
    "    print(f\"top{k}:\", topk_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e512d7-7fa4-40d2-97c1-8c1e0380956c",
   "metadata": {},
   "source": [
    "### 4.2 check badcase\n",
    "因为我觉得top100都无法召回的问题，可能是低质量问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fea3c1b4-f744-4b60-90f2-e93baa4e2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"发动机内部清洗油有必要吗\",\n",
      "        \"\"\n",
      "    ],\n",
      "    [\n",
      "        \"太阳穴针扎一样疼\",\n",
      "        \"\"\n",
      "    ],\n",
      "    [\n",
      "        \"王者战士制裁的用处\",\n",
      "        \"\"\n",
      "    ],\n",
      "    [\n",
      "        \"高阳膝盖不好使疼痛甚是是什么引起的\",\n",
      "        \"\"\n",
      "    ],\n",
      "    [\n",
      "        \"什么情况会被拉入黑名单买不了车票\",\n",
      "        \"\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "# check qa quality\n",
    "index = [randrange(len(qa_dataset_with_retrieval)) for i in range(5)]\n",
    "small_dataset = qa_dataset_with_retrieval.select(index).remove_columns([\"retrieved_docids\"])\n",
    "print(json.dumps([(i[\"question\"], i[\"answer\"]) for i in small_dataset], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2bbf2a97-543b-4bf8-848c-1a8744cd0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_badcase(questions, predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    bad_cases = []\n",
    "    for ques, pred, true_label in zip(questions, predictions, true_labels):\n",
    "        if true_label not in pred[:topk]:\n",
    "            bad_cases.append(ques)\n",
    "    return bad_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65d8a741-7105-42c5-9f4d-7b99f8a5e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182074 ['鹦鹉的生活习性是什么', '鹦鹉的生活习性是什么', '鹦鹉鱼头洞传染吗', '鹰潭市景点排行榜', '鹰潭市景点排行榜', '鹰嘴骨折手术后能正常生活吗', '鹰嘴骨折手术后能正常生活吗', '鹰嘴骨折手术后能正常生活吗', '鹰嘴骨折手术后能正常生活吗', '盈利的盈组词']\n"
     ]
    }
   ],
   "source": [
    "top100_bad_case = check_badcase(questions, predictions, targets, topk=100)\n",
    "print(len(top100_bad_case), top100_bad_case[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c23001c-bc9e-48ce-982d-b97d1430577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17707 ['鹦鹉吃自己的小鱼吗', '荧光pcr基因 内参关系', '硬路肩也铺沥青的吗', '用不完的尿不湿的妙用', '用电饼档爆栗子怎么样', '用电饼档爆栗子怎么样', '用两个抢票软件可以吗', '用硫磺皂洗澡后有气味', '用人单位还是派遣单位工资', '用珊瑚癣净泡脚非常疼']\n"
     ]
    }
   ],
   "source": [
    "top50_bad_case = check_badcase(questions, predictions, targets, topk=50)\n",
    "top100_badcase_set = set(top100_bad_case)\n",
    "top50_bad_case_diff = [q for q in top50_bad_case if q not in top100_badcase_set]\n",
    "print(len(top50_bad_case_diff), top50_bad_case_diff[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cce408cd-321a-4833-b89c-fac067ace888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307701 27614 ['电动车电池测试好坏', '网购哪个平台能买到正品衣服', '咸阳晚上哪里好玩的景点', '打孩子多久子宫就好了', '单株是什么意思', '儿童便秘应该挂什么科', '生育难免险流产可以报销多少', '汽车尿素液起什么作用', '邮政快递为什么这么慢', '非洲旅游哪里安全']\n"
     ]
    }
   ],
   "source": [
    "top20_bad_case = check_badcase(questions, predictions, targets, topk=20)\n",
    "top50_badcase_set = set(top50_bad_case)\n",
    "top20_bad_case_diff = [q for q in top20_bad_case if q not in top50_badcase_set]\n",
    "top20_bad_case_diff_sampled = [top20_bad_case_diff[randrange(len(top20_bad_case_diff))] for i in range(10)]\n",
    "print(len(top20_bad_case), len(top20_bad_case_diff), top20_bad_case_diff_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a1567-d7f9-481f-ab6a-a675d26840ab",
   "metadata": {},
   "source": [
    "##### 观察： top20未召回的问题，主要是召回模型的问题，感觉可以不用过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a9397-e774-4c94-ad86-8c0623f671c2",
   "metadata": {},
   "source": [
    "## 5. Filtering\n",
    "T2Ranking这个数据集，就不过滤了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bb59b5",
   "metadata": {},
   "source": [
    "不过滤重复问题，因为数据集天然就是1对多的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c7175",
   "metadata": {},
   "source": [
    "## 6. save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "30817850-c205-424d-98bd-d46836282a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2731227564244c66967200ce5bca15ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/740960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 不train_test_split\n",
    "qa_dataset_with_retrieval_output_path = \"data/T2Ranking/T2Ranking_train_dataset\"\n",
    "qa_dataset_with_retrieval.save_to_disk(qa_dataset_with_retrieval_output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e7a37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13c52c9-d093-4800-ad05-efcf9cf48967",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58709ad-e2cd-4cb4-88f9-72b991aa2126",
   "metadata": {},
   "source": [
    "### 1.1 Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c67427-5804-47b0-952b-214052bd5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "!pip install sentence_transformers datasets\n",
    "# if you have gpu, install faiss gpt\n",
    "!pip install faiss-gpu\n",
    "# else instlal faiss cpu\n",
    "# !pip install faiss-cpu\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89ac1c-bf6c-48aa-843e-5ec2567bab5a",
   "metadata": {},
   "source": [
    "### 1.2 Setup Retriever\n",
    "here we use m3e-base as the chinese Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de261818-aad3-4a29-b832-6ad2a74c48b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/howard/miniconda3/envs/torch1.13/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/howard/miniconda3/envs/torch1.13 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/local/cuda/lib64: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('{\"locale\"'), PosixPath('{\"*\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('true}'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"fe9960b3a97de5d66c86c362d68dbccd.zh-cn\",\"_translationsConfigFile\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/fe9960b3a97de5d66c86c362d68dbccd.zh-cn/6c3e3dba23e8fadc360aed75ce363ba185c49794\",\"_corruptedFile\"')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: * Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练\n",
      "Embedding: (768,)\n",
      "\n",
      "Sentence: * Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one\n",
      "Embedding: (768,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('moka-ai/m3e-base')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = [\n",
    "    '* Moka 此文本嵌入模型由 MokaAI 训练并开源，训练脚本使用 uniem',\n",
    "    '* Massive 此文本嵌入模型通过**千万级**的中文句对数据集进行训练',\n",
    "    '* Mixed 此文本嵌入模型支持中英双语的同质文本相似度计算，异质文本检索等功能，未来还会支持代码检索，ALL in one'\n",
    "]\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2db9c1-8648-4e1e-abda-c96d114beccf",
   "metadata": {},
   "source": [
    "## 2. Preprocess QA data\n",
    "### 2.1 clean qa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ce3086-1239-45fc-a0b8-bfbbc237b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_special_char(pattern, text, src=\"\\n\\n\", tgt=\"\\n\"):\n",
    "    while True:\n",
    "        i = re.search(pattern, text)\n",
    "        if i is None:\n",
    "            break\n",
    "        s, e = i.span()\n",
    "        # split text into 3 parts\n",
    "        q, a, b = text[:s], text[s:e], text[e:]\n",
    "        a = a.replace(src, tgt)\n",
    "        # merge 3 parts\n",
    "        text = q + a + b\n",
    "    return text\n",
    "\n",
    "def split_text(text):\n",
    "    # 处理特殊情况：Q和A分隔符也是\\n\\n, 替换为\\n\n",
    "    text = replace_special_char(\"Q: .*?\\n\\nA:\", text)\n",
    "    text = replace_special_char(\"Q[0-9]+: .*?\\n\\nA[0-9]+:\", text)\n",
    "    text = replace_special_char(\"[0-9]+\\. .*?\\n\\n答：\", text)\n",
    "    text = replace_special_char(\"[0-9]+\\. .*?\\n\\n\", text)\n",
    "    # 处理特殊情况：QA和QA分隔符是\\n, 替换为\\n\\n\n",
    "    text = replace_special_char('(?<!\\n)\\n\\d+\\.', text, src=\"\\n\", tgt=\"\\n\\n\")\n",
    "    qa_pairs = [qa for qa in text.split('\\n\\n') if '\\n' in qa]\n",
    "    return qa_pairs\n",
    "\n",
    "def clean_qa(q, a):\n",
    "    # Remove \"1.\" at the beginning of the sentence\n",
    "    cleaned_q = re.sub(r'^\\d+\\.\\s+', '', q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^Q\\d+:\\s+', '', cleaned_q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^Q:\\s+', '', cleaned_q, flags=re.MULTILINE)\n",
    "    cleaned_q = re.sub(r'^> \\d+\\. ', '', cleaned_q)\n",
    "    \n",
    "    # Remove \"答：\" at the beginning of the sentence\n",
    "    cleaned_a = re.sub(r'^答：', '', a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^A\\d+:\\s+', '', cleaned_a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^A:\\s+', '', cleaned_a, flags=re.MULTILINE)\n",
    "    cleaned_a = re.sub(r'^-\\s+', '', cleaned_a)\n",
    "    cleaned_a = re.sub(r'^回答：', '', cleaned_a)\n",
    "    \n",
    "    return cleaned_q.strip(), cleaned_a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb6904a-f1cc-4d53-a5c6-868ebb6ef7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_to_12_length(input_string):\n",
    "    # Create an MD5 hash object\n",
    "    md5_hash = hashlib.md5()\n",
    "    \n",
    "    # Convert the input string to bytes and update the hash object\n",
    "    md5_hash.update(input_string.encode('utf-8'))\n",
    "    \n",
    "    # Get the hexadecimal representation of the hash\n",
    "    hashed_string = md5_hash.hexdigest()\n",
    "    \n",
    "    # Truncate the hash to 12 characters\n",
    "    truncated_hash = hashed_string[:12]\n",
    "    \n",
    "    return truncated_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b7ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_ngram(text, n=2):\n",
    "    # 创建 n-gram 列表\n",
    "    ngrams = [\"\".join(text[i:i+n]) for i in range(len(text) - n + 1)]\n",
    "    # 统计 n-gram 出现次数\n",
    "    ngram_counter = Counter(ngrams)\n",
    "    # 计算总单词数\n",
    "    total_words = len(ngrams)\n",
    "    # 计算每个 n-gram 的频率\n",
    "    ngram_freq = {ngram: ngram_counter[ngram] / total_words for ngram in ngram_counter}\n",
    "    return ngram_freq\n",
    "\n",
    "def is_repeated_text(text, threshold=0.1, n=2):\n",
    "    # 计算词组频率\n",
    "    ngram_freq = count_ngram(text, n)\n",
    "    # 如果 max(ngram_freq.values()) 大于阈值，则返回False\n",
    "    return max(ngram_freq.values()) > threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3637f787-7d1e-492e-a092-5317df844186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "dureader_dir = Path(\"data/dureader\")\n",
    "dureader_dir.mkdir(exist_ok=True)\n",
    "qa_input_path = \"data/raw_data/train_dureader_cleaned.jsonl\"\n",
    "qa_ouput_path = dureader_dir / \"dureader-qapairs_all.json\"\n",
    "document_path = dureader_dir / \"dureader-documents_all.json\"\n",
    "\n",
    "# 1. build documents\n",
    "docid2doc = {}\n",
    "for line in open(qa_input_path):\n",
    "    doc = json.loads(line)[\"materials\"]\n",
    "    docid = hash_to_12_length(doc)\n",
    "    docid2doc[docid] = doc\n",
    "documents = []\n",
    "for docid, doc in docid2doc.items():\n",
    "    item = dict(docid=docid, document=doc, source=\"dureader\")\n",
    "    documents.append(item)\n",
    "json.dump(documents, open(document_path, \"w\"), ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f52f167d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filter, qa pairs num: 162661\n",
      "after filter, qa pairs num: 159893\n",
      "drop qa pairs num ratio: 0.017016986247471735\n",
      "max ans len in keep: 141\n",
      "min ans len in keep: 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. build qa pairs\n",
    "qa_pairs_with_docid = []\n",
    "ans_lens = []\n",
    "for line in open(qa_input_path):\n",
    "    j = json.loads(line)\n",
    "    qa_pairs = j[\"conversation\"]\n",
    "    doc = j[\"materials\"].strip()\n",
    "    docid = hash_to_12_length(doc)\n",
    "    for i, qa_pair in enumerate(qa_pairs):\n",
    "        q = qa_pair.get(\"QUES\") if \"QUES\" in qa_pair else qa_pair.get(\"QUS\")\n",
    "        a = qa_pair.get(\"ANS\") if \"ANS\" in qa_pair else qa_pair.get(\"AAS\")\n",
    "        if q and a:\n",
    "            ans_lens.append(len(a))\n",
    "            item = dict(question=q, answer=a, docid=docid)\n",
    "            qa_pairs_with_docid.append(item)\n",
    "        \n",
    "# 3. statistics and filter\n",
    "print(\"before filter, qa pairs num:\", len(qa_pairs_with_docid))\n",
    "qa_pairs_with_docid_keep = []\n",
    "qa_pairs_with_docid_drop = []\n",
    "ans_len_percentile99 = np.percentile(ans_lens, 99)\n",
    "ans_len_percentile1 = np.percentile(ans_lens, 1)\n",
    "ans_len_percentile95 = np.percentile(ans_lens, 95)\n",
    "for item in qa_pairs_with_docid:\n",
    "    if (len(item[\"answer\"]) >= ans_len_percentile1 and \n",
    "        len(item[\"answer\"]) <= ans_len_percentile99):\n",
    "\n",
    "        if (len(item[\"answer\"]) > ans_len_percentile95 and \n",
    "            is_repeated_text(item[\"answer\"], threshold=0.10)):\n",
    "\n",
    "            qa_pairs_with_docid_drop.append(item)\n",
    "        else:\n",
    "            qa_pairs_with_docid_keep.append(item)\n",
    "    else:\n",
    "        qa_pairs_with_docid_drop.append(item)\n",
    "print(\"after filter, qa pairs num:\", len(qa_pairs_with_docid_keep))\n",
    "print(\"drop qa pairs num ratio:\", len(qa_pairs_with_docid_drop) / len(qa_pairs_with_docid))\n",
    "print(\"max ans len in keep:\", max([len(item[\"answer\"]) for item in qa_pairs_with_docid_keep]))\n",
    "print(\"min ans len in keep:\", min([len(item[\"answer\"]) for item in qa_pairs_with_docid_keep]))\n",
    "\n",
    "json.dump(qa_pairs_with_docid_keep, open(qa_ouput_path, \"w\"), ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09605b00-4269-45d4-986a-6d050a77fdbe",
   "metadata": {},
   "source": [
    "### 2.2 build faiss index for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5382346d-a200-4a51-bcf7-d6fd13752560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['docid', 'document', 'source'],\n",
      "    num_rows: 11454\n",
      "})\n",
      "{\n",
      "  \"docid\": \"538aa9eccd44\",\n",
      "  \"document\": \"选择燃气热水器时，一定要关注这几个问题：1、出水稳定性要好，不能出现忽热忽冷的现象2、快速到达设定的需求水温3、操作要智能、方便4、安全性要好，要装有安全报警装置 市场上燃气热水器品牌众多，购买时还需多加对比和仔细鉴别。方太今年主打的磁化恒温热水器在使用体验方面做了全面升级：9秒速热，可快速进入洗浴模式；水温持久稳定，不会出现忽热忽冷的现象，并通过水量伺服技术将出水温度精确控制在±0.5℃，可满足家里宝贝敏感肌肤洗护需求；配备CO和CH4双气体报警装置更安全（市场上一般多为CO单气体报警）。另外，这款热水器还有智能WIFI互联功能，只需下载个手机APP即可用手机远程操作热水器，实现精准调节水温，满足家人多样化的洗浴需求。当然方太的磁化恒温系列主要的是增加磁化功能，可以有效吸附水中的铁锈、铁屑等微小杂质，防止细菌滋生，使沐浴水质更洁净，长期使用磁化水沐浴更利于身体健康。\",\n",
      "  \"source\": \"dureader\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# document_dataset = load_dataset(\"json\", data_files=document_path)\n",
    "document_dataset = Dataset.from_list(documents)\n",
    "print(document_dataset)\n",
    "print(json.dumps(document_dataset[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecd4eb2c-6f7e-4888-bcd9-30c892395a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17757e62e7e54ac688c0b9cafadd9b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64f9d74972449418e6a9740d1366684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "document_dataset_with_emb = document_dataset.map(\n",
    "    lambda example: {'doc_embedding': model.encode(example[\"document\"])}, \n",
    "    batched=True\n",
    ")\n",
    "# 模型index在cpu，设置device可以把index放到对应gpu，#device is the index of the GPU to use !!!但有非常慢\n",
    "document_dataset_with_emb.add_faiss_index(column='doc_embedding')\n",
    "document_dataset_with_emb.save_faiss_index('doc_embedding', 'data/raw_data/dureader_doc_embedding.faiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feeaa53-2db3-4f80-9adf-506e9736a072",
   "metadata": {},
   "source": [
    "### 2.3 encode questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "809787f1-d6dc-4cbe-b730-311c66f23bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid'],\n",
      "    num_rows: 159893\n",
      "})\n",
      "{'question': '选择燃气热水器时，需要关注哪些问题？', 'answer': '选购燃气热水器时，需要关注以下几个问题：1、出水稳定性好，不能出现忽热忽冷的现象。2、快速到达设定的需求水温。3、操作智能方便。4、安全性要好，要装有安全报警装置。', 'docid': '538aa9eccd44'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c04d8db43b4dd8aa23a7b4709fa28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# qa_dataset = load_dataset(\"json\", data_files=\"wikipedia-cn-20230720-qapairs_10k.json\")\n",
    "qa_dataset = Dataset.from_list(qa_pairs_with_docid_keep)\n",
    "print(qa_dataset)\n",
    "print(qa_dataset[0])\n",
    "# \n",
    "qa_dataset_with_emb = qa_dataset.map(\n",
    "    lambda example: {'question_embedding': model.encode(example[\"question\"])}, \n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50022e28-58cf-4fa8-9052-39454a42e206",
   "metadata": {},
   "source": [
    "## 3. Build dataset\n",
    "### 3.1 retrieve relevant documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7d157ff-75e0-48b8-b636-71a681430c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def retrieve_topk_documents(example):\n",
    "    topk = 100\n",
    "    ques_embedding = np.array(example[\"question_embedding\"], dtype=np.float32)\n",
    "    scores, retrieved_examples = document_dataset_with_emb.get_nearest_examples('doc_embedding', ques_embedding, k=topk)\n",
    "    example[\"retrieved_docids\"] = retrieved_examples[\"docid\"]\n",
    "    example[\"retrieved_doc_scores\"] = scores.tolist()\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77ebd94a-8301-4fb2-bbd7-4144ef4b280b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b4a1d495f94d0086f101a5dd66d7e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/159893 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 根据硬件，调整num_proc\n",
    "qa_dataset_with_retrieval = qa_dataset_with_emb.map(retrieve_topk_documents, \n",
    "                                                    num_proc=16,\n",
    "                                                    remove_columns=[\"question_embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc74bd4-0869-4ecb-89de-1997acc308b0",
   "metadata": {},
   "source": [
    "## 4. Evaluate\n",
    "### 4.1 Evaluate retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b87f3f3b-bff8-4ef7-abe0-d1785b585666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    for pred, true_label in zip(predictions, true_labels):\n",
    "        if true_label in pred[:topk]:\n",
    "            num_correct += 1\n",
    "    \n",
    "    top1_accuracy = num_correct / len(predictions)\n",
    "    return top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "525e4540-8e9a-41b9-af90-47a2e89c7407",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159893/159893 [00:29<00:00, 5388.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "questions, predictions, targets = [], [], []\n",
    "for s in tqdm(qa_dataset_with_retrieval):\n",
    "    questions.append(s[\"question\"])\n",
    "    predictions.append(s[\"retrieved_docids\"])\n",
    "    targets.append(s[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45558dc7-44fc-42df-a6f6-cc9c0b3b118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top1: 0.7172484098741033\n",
      "top3: 0.854408885942474\n",
      "top5: 0.8917088302802499\n",
      "top10: 0.9261193423101699\n",
      "top20: 0.9500103194010995\n",
      "top50: 0.9712933023959773\n",
      "top100: 0.9820504962693801\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "    topk_acc = compute_topk_accuracy(predictions, targets, topk=k)\n",
    "    print(f\"top{k}:\", topk_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e512d7-7fa4-40d2-97c1-8c1e0380956c",
   "metadata": {},
   "source": [
    "### 4.2 check badcase\n",
    "因为我觉得top100都无法召回的问题，可能是低质量问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fea3c1b4-f744-4b60-90f2-e93baa4e2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"志高的服务电话是多少？\",\n",
      "        \"志高的服务电话是0571-8513-5103。\"\n",
      "    ],\n",
      "    [\n",
      "        \"排球场地有哪些规格类型可选？\",\n",
      "        \"排球场地可选择全塑(QS)型或混合(HH)型，颜色可以是铁红、草绿或根据用户需求定制，厚度一般为7-10mm或按用户要求定制。\"\n",
      "    ],\n",
      "    [\n",
      "        \"什么是怀孕最显著也是最早的信号？\",\n",
      "        \"月经停止是怀孕最显著也是最早的一个信号，如果在无避孕措施下进行了性生活而出现月经停止的话，很可能就是怀孕了。\"\n",
      "    ],\n",
      "    [\n",
      "        \"无创DNA检测的价格大概在多少范围内？\",\n",
      "        \"无创DNA检测的价格大约在2000到2800之间。\"\n",
      "    ],\n",
      "    [\n",
      "        \"《择天记》的首播时间是什么时候？\",\n",
      "        \"《择天记》于2017年4月17日登陆湖南卫视“青春进行时”时段开始播出。\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from random import randrange\n",
    "# check qa quality\n",
    "index = [randrange(len(qa_dataset_with_retrieval)) for i in range(5)]\n",
    "small_dataset = qa_dataset_with_retrieval.select(index).remove_columns([\"retrieved_docids\"])\n",
    "print(json.dumps([(i[\"question\"], i[\"answer\"]) for i in small_dataset], ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bbf2a97-543b-4bf8-848c-1a8744cd0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_badcase(questions, predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    bad_cases = []\n",
    "    for ques, pred, true_label in zip(questions, predictions, true_labels):\n",
    "        if true_label not in pred[:topk]:\n",
    "            bad_cases.append(ques)\n",
    "    return bad_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65d8a741-7105-42c5-9f4d-7b99f8a5e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2870 ['新闻来源提到了哪个娱乐频道网站？', '请问您的手机是什么型号的呢?', '售后服务中心的地址和联系电话在哪里能查询到？', '请问您的手机是什么型号的呢?', '什么是四维检查？', '为什么需要进行四维检查？', '为什么选医院要考虑服务态度和正规性？', '为什么要近水楼台先得月？', '如果术前有炎症，是否需要先治好才能做手术？', '这个剧情是根据哪部作品改编的？']\n"
     ]
    }
   ],
   "source": [
    "top100_bad_case = check_badcase(questions, predictions, targets, topk=100)\n",
    "print(len(top100_bad_case), top100_bad_case[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c23001c-bc9e-48ce-982d-b97d1430577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704 ['电视剧《斗破苍穹》预计何时上映播出？', '电视剧《斗破苍穹》预计何时上映播出？', '电视剧《斗破苍穹》何时上映播出？', '请问您的手机是什么型号的呢？', '什么是阻击模式，如何进入该模式？', '需要提前购买回程的车票吗？', '驾车路线中需要转弯几次？', 'mAh和mA之间有什么关系？', '沿着哪些地点可以到达长江路淮南街站？', '还有其他类似的软件吗？']\n"
     ]
    }
   ],
   "source": [
    "top50_bad_case = check_badcase(questions, predictions, targets, topk=50)\n",
    "top50_bad_case_diff = [q for q in top50_bad_case if q not in top100_bad_case]\n",
    "print(len(top50_bad_case_diff), top50_bad_case_diff[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cce408cd-321a-4833-b89c-fac067ace888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7993 3331 ['这首歌的演唱者是谁？', '可以从一个半岛入境再从另一个半岛出境吗？', '如何在《龙魂》中获得大礼包？', '2017年3月8日至2017年3月14日期间有什么活动？', '22寸行李箱的尺寸是多少？', '更换空调器压缩机时需要注意什么？', '一升车预计何时上市？', '有什么建议可以查看三本院校的招生简章吗？', '生化危机6还有其他的游戏模式吗？', '题记中描述了什么样的情感？']\n"
     ]
    }
   ],
   "source": [
    "top20_bad_case = check_badcase(questions, predictions, targets, topk=20)\n",
    "top20_bad_case_diff = [q for q in top20_bad_case if q not in top50_bad_case]\n",
    "top20_bad_case_diff_sampled = [top20_bad_case_diff[randrange(len(top20_bad_case_diff))] for i in range(10)]\n",
    "print(len(top20_bad_case), len(top20_bad_case_diff), top20_bad_case_diff_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a1567-d7f9-481f-ab6a-a675d26840ab",
   "metadata": {},
   "source": [
    "##### 观察： top20未召回的问题，确实不少是低质量问题，也有一些是比较难的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355a9397-e774-4c94-ad86-8c0623f671c2",
   "metadata": {},
   "source": [
    "## 5. Filtering\n",
    "保留top20内能召回正确文档的问题, 过滤重复问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "737085c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
       "    num_rows: 135333\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qa_df = pd.DataFrame(qa_dataset_with_retrieval)\n",
    "qa_df.drop_duplicates(subset=['question'], inplace=True)\n",
    "qa_dataset_dedup = Dataset.from_pandas(qa_df).remove_columns(['__index_level_0__'])\n",
    "qa_dataset_dedup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9fc4f3af-6801-4932-bede-474d873afdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513e5949e1cf43809c7b868c0463d37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/135333 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "    num_rows: 128224\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "qa_dataset_filtered = qa_dataset_dedup.filter(lambda x: x[\"question\"] not in top20_bad_case)\n",
    "print(qa_dataset_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c7175",
   "metadata": {},
   "source": [
    "## 6. Train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30817850-c205-424d-98bd-d46836282a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "        num_rows: 126941\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "        num_rows: 1283\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590f8130912748f6a2ebaa74e3062c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/126941 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1adbb3abd8594ccc9f683d5214c7bf34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1283 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qa_dataset_with_retrieval_output_path = \"data/dureader/dureader_dataset\"\n",
    "qa_dataset_train_test = qa_dataset_filtered.train_test_split(test_size=0.01, shuffle=True)\n",
    "print(qa_dataset_train_test)\n",
    "qa_dataset_train_test.save_to_disk(qa_dataset_with_retrieval_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cbe7c6",
   "metadata": {},
   "source": [
    "## 7. evaluate retrieval model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df13b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'docid', 'retrieved_docids', 'retrieved_doc_scores'],\n",
      "    num_rows: 1283\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/torch1.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "test_ds = load_from_disk(\"data/dureader/dureader_dataset/\")[\"test\"]\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8df05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from metric import RetrievalMetrics\n",
    "from misc import recursive_round\n",
    "\n",
    "def evaluate_retriever(dataset):\n",
    "    retrieval_metrics = RetrievalMetrics()\n",
    "    for example in dataset:\n",
    "        # ranking metric\n",
    "        rank_preds = [1 / (1+p) for p in example[\"retrieved_doc_scores\"]]\n",
    "        rank_targets = [False] * len(rank_preds)\n",
    "        pos_idx =  example[\"retrieved_docids\"].index(example[\"docid\"])\n",
    "        rank_targets[pos_idx] = True\n",
    "        retrieval_metrics.update([rank_preds], [rank_targets])\n",
    "    return recursive_round(retrieval_metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e9d0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"HitRate@1\": 0.761,\n",
      "  \"HitRate@5\": 0.933,\n",
      "  \"HitRate@10\": 0.971,\n",
      "  \"MRR\": 0.838,\n",
      "  \"MAP@1\": 0.761,\n",
      "  \"MAP@5\": 0.83,\n",
      "  \"MAP@10\": 0.836,\n",
      "  \"NDCG@1\": 0.761,\n",
      "  \"NDCG@5\": 0.856,\n",
      "  \"NDCG@10\": 0.869,\n",
      "  \"Recall@1\": 0.761,\n",
      "  \"Recall@5\": 0.933,\n",
      "  \"Recall@10\": 0.971\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "eval_results = evaluate_retriever(test_ds)\n",
    "print(json.dumps(eval_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8720378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_topk_accuracy(predictions, true_labels, topk=5):\n",
    "    assert len(predictions) == len(true_labels), \"预测结果和真实标签的数量必须相同\"\n",
    "    \n",
    "    num_correct = 0\n",
    "    for pred, true_label in zip(predictions, true_labels):\n",
    "        if true_label in pred[:topk]:\n",
    "            num_correct += 1\n",
    "    \n",
    "    top1_accuracy = num_correct / len(predictions)\n",
    "    return top1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26e16123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1283/1283 [00:00<00:00, 13866.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "questions, predictions, targets = [], [], []\n",
    "for s in tqdm(test_ds):\n",
    "    questions.append(s[\"question\"])\n",
    "    predictions.append(s[\"retrieved_docids\"])\n",
    "    targets.append(s[\"docid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f1f59d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1: 0.7614964925954794\n",
      "Recall@3: 0.8916601714731099\n",
      "Recall@5: 0.9329696024941543\n",
      "Recall@10: 0.9711613406079501\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10]:\n",
    "    topk_acc = compute_topk_accuracy(predictions, targets, topk=k)\n",
    "    print(f\"Recall@{k}:\", topk_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627105d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf6cb17e-af13-41b5-a69a-0effbb004160",
   "metadata": {},
   "source": [
    "## 1. init model\n",
    "### 1.1 config model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207c5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a76da14f-1eb6-4a4b-b350-18b5daa51400",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "ERROR: /home/howard/miniconda3/envs/torch1.13/bin/python: undefined symbol: cudaRuntimeGetVersion\n",
      "CUDA SETUP: libcudart.so path is None\n",
      "CUDA SETUP: Is seems that your cuda installation is not in your path. See https://github.com/TimDettmers/bitsandbytes/issues/85 for more information.\n",
      "CUDA SETUP: CUDA version lower than 11 are currently not supported for LLM.int8(). You will be only to use 8-bit optimizers and quantization routines!!\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 00\n",
      "CUDA SETUP: Loading binary /home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cextension.py:33: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /home/howard/miniconda3/envs/torch1.13 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/local/cuda/lib64: did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('Asia/Shanghai')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('\"zh-cn\",\"availableLanguages\"'), PosixPath('\"zh-cn\"},\"_languagePackId\"'), PosixPath('{\"*\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/73405bc210cb79e0045de494d8fc24af.zh-cn/tcf.json\",\"_cacheRoot\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/73405bc210cb79e0045de494d8fc24af.zh-cn/corrupted.info\",\"_languagePackSupport\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/73405bc210cb79e0045de494d8fc24af.zh-cn/6445d93c81ebe42c4cbd7a60712e0b17d9463e97\",\"_corruptedFile\"'), PosixPath('{\"locale\"'), PosixPath('\"zh-cn\",\"osLocale\"'), PosixPath('\"/home/howard/.vscode-server/data/clp/73405bc210cb79e0045de494d8fc24af.zh-cn\",\"_resolvedLanguagePackCoreLocation\"'), PosixPath('\"73405bc210cb79e0045de494d8fc24af.zh-cn\",\"_translationsConfigFile\"'), PosixPath('true}')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: No libcudart.so found! Install CUDA or the cudatoolkit package (anaconda)!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from modeling_rankprompter import RankPrompter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "prompter_tokenizer = AutoTokenizer.from_pretrained(\"google/umt5-small\")\n",
    "prompter_config = AutoConfig.from_pretrained(\"google/umt5-small\")\n",
    "# baichuan\n",
    "language_model_config = AutoConfig.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\n",
    "language_model_tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\", trust_remote_code=True)\n",
    "language_model_tokenizer.pad_token_id = language_model_config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f78b072-62ca-4500-9d65-c40ddf4058be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompter trainable params: 0.44B || all params: 0.44B || trainable%: 100.0000\n"
     ]
    }
   ],
   "source": [
    "from misc import count_parameters\n",
    "prompter_config.num_soft_prompt_tokens = 32\n",
    "prompter_config.llm_dim = language_model_config.hidden_size\n",
    "prompter = RankPrompter(prompter_config).to(device)\n",
    "trainable_params, all_param = count_parameters(prompter)\n",
    "print(\"prompter trainable params: {:.2f}B || all params: {:.2f}B || trainable%: {:.4f}\".format(\n",
    "        trainable_params / 1e9, all_param / 1e9, 100 * trainable_params / all_param\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a88e1-5413-4cf4-90a0-8feea572085f",
   "metadata": {},
   "source": [
    "## 2. Init Dataset\n",
    "### 2.1 load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b2379d-958d-4f19-9bd7-33a08da5b156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/howard/miniconda3/envs/torch1.13/lib/python3.10/site-packages/datasets/dataset_dict.py:1241: FutureWarning: 'fs' was is deprecated in favor of 'storage_options' in version 2.8.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'storage_options=fs.storage_options' instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_from_disk\n",
    "\n",
    "document_path = \"wikipedia-cn-20230720-documents_10k.json\"\n",
    "qa_path = \"wikipedia-cn-20230720_qa-with-retrieval_10k/\"\n",
    "\n",
    "docid2doc = {d[\"docid\"]:d[\"document\"] for d in json.load(open(document_path))}\n",
    "\n",
    "qa_dataset = load_from_disk(qa_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bdeb0b-25c4-484f-b259-1ae5c7695e5d",
   "metadata": {},
   "source": [
    "### 2.2 tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34fb47da-b921-46d4-ae31-51303b687b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(example):\n",
    "    # \n",
    "    num_doc = 20\n",
    "    doc_max_length = 512\n",
    "    ques_max_length = 32\n",
    "    ans_max_length = 128\n",
    "    #\n",
    "    pos_docid = example[\"docid\"]\n",
    "    # put pos_docid in the first place\n",
    "    docids = [pos_docid] + [docid for docid in example[\"retrieved_docids\"] if docid != pos_docid]\n",
    "    docs = [docid2doc[docid] for docid in docids[:num_doc]]\n",
    "    # padding to specific length, make all example have the same shape\n",
    "    prompter_tokenzied_docs = prompter_tokenizer(docs, padding=\"max_length\", \n",
    "                                                truncation=True, max_length=doc_max_length)\n",
    "    prompter_tokenzied_question = prompter_tokenizer(example[\"question\"], padding=\"max_length\", \n",
    "                                                truncation=True, max_length=ques_max_length)\n",
    "    prompter_tokenzied_answer = prompter_tokenizer(example[\"answer\"], padding=\"max_length\", \n",
    "                                                truncation=True, max_length=ans_max_length)\n",
    "    language_model_tokenzied_question = language_model_tokenizer(example[\"question\"], padding=\"max_length\",\n",
    "                                                truncation=True, max_length=ques_max_length)\n",
    "    language_model_tokenzied_answer = language_model_tokenizer(example[\"answer\"], padding=\"max_length\",\n",
    "                                                truncation=True, max_length=ans_max_length)\n",
    "    return {\"document_input_ids\": prompter_tokenzied_docs.input_ids,\n",
    "            \"document_attention_mask\": prompter_tokenzied_docs.attention_mask,\n",
    "            \"prompter_question_input_ids\": prompter_tokenzied_question.input_ids,\n",
    "            \"prompter_question_attention_mask\": prompter_tokenzied_question.attention_mask,\n",
    "            \"prompter_answer_input_ids\": prompter_tokenzied_answer.input_ids,\n",
    "            \"prompter_answer_attention_mask\": prompter_tokenzied_answer.attention_mask,\n",
    "            \"language_model_question_input_ids\": language_model_tokenzied_question.input_ids,\n",
    "            \"language_model_question_attention_mask\": language_model_tokenzied_question.attention_mask,\n",
    "            \"language_model_answer_input_ids\": language_model_tokenzied_answer.input_ids,\n",
    "            \"language_model_answer_attention_mask\": language_model_tokenzied_answer.attention_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408250be-5bde-4acf-b80f-c315c91eea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-74e2008293004796.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-fb09bec8ab5a6c4a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-691dc1b90a74e5f1.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-add749843132a27f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-594bfa22304c566d.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-919b159bb6e2f7f9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-6318c30b23dea7c3.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-05a4d046a66b9b2c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-c19c732dc40e31dc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-8c42a1ff33f35aaa.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-b0d06cc8ea3330a0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-76653ed279e8d08f.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-b1f0f132646e8287.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-3857c8b0f80994d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-8a20cdf5f38e6d7c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/train/cache-a1cd63a5c784a3b0.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-bcdcdc7e268e3f5f.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-081f458ec29b8c58.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-c9a9809ee3b0ef2d.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-08522d06b722d3c2.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-581d0217323ab761.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-fe1c929c32aa2f7a.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-1f046ef237a8175c.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-09d278575afaf9cf.arrow\n",
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-608fc1f2a7cbe5ec.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-25f49b7325ee34a2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-2f383080372e8afd.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-d186407a847a120b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-4282e34bdcacd3c4.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-189ba4d91900efe9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-9698ba4c6c4c7e57.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /data/Documents/MyCode/RankPrompter/wikipedia-cn-20230720_qa-with-retrieval_10k/test/cache-53216c8f5accc117.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_qa_dataset = qa_dataset.map(preprocess_dataset, \n",
    "                                    num_proc=16, \n",
    "                                    remove_columns=[\"retrieved_docids\"]).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0bf7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'docid', 'document_input_ids', 'document_attention_mask', 'prompter_question_input_ids', 'prompter_question_attention_mask', 'prompter_answer_input_ids', 'prompter_answer_attention_mask', 'language_model_question_input_ids', 'language_model_question_attention_mask', 'language_model_answer_input_ids', 'language_model_answer_attention_mask'],\n",
       "        num_rows: 55192\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'docid', 'document_input_ids', 'document_attention_mask', 'prompter_question_input_ids', 'prompter_question_attention_mask', 'prompter_answer_input_ids', 'prompter_answer_attention_mask', 'language_model_question_input_ids', 'language_model_question_attention_mask', 'language_model_answer_input_ids', 'language_model_answer_attention_mask'],\n",
       "        num_rows: 6133\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82030a-a6cb-45cd-9ec0-df82688c439e",
   "metadata": {},
   "source": [
    "### 2.3 init dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c2b1977-a757-4d7d-a7f4-d9bfec96249b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "# Create a DataLoader with the desired batch size\n",
    "batch_size = 2 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "gradient_accumulation_steps = 8 # accumulate gradients over n batches\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_qa_dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(tokenized_qa_dataset[\"test\"], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ba2c3",
   "metadata": {},
   "source": [
    "## 3. train\n",
    "### 3.1 config optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddba016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: False\n",
      "num_training_steps: 10348\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers import get_polynomial_decay_schedule_with_warmup\n",
    "# optimizer config \n",
    "learning_rate = 1e-4\n",
    "# Create AdamW optimizer and use the fused version if it is available\n",
    "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "use_fused = fused_available and device == 'cuda'\n",
    "extra_args = dict(fused=True) if use_fused else dict()\n",
    "optimizer = torch.optim.AdamW(prompter.parameters(), lr=learning_rate, **extra_args)\n",
    "print(f\"using fused AdamW: {use_fused}\")\n",
    "# scheduler config\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader) // gradient_accumulation_steps\n",
    "lr_scheduler = get_polynomial_decay_schedule_with_warmup(\n",
    "    optimizer=optimizer,  # scheduler是针对optimizer的lr的\n",
    "    lr_end=1e-7,\n",
    "    power=1, # 当power=1时（默认）等价于linear_schedule_with_warmup\n",
    "    num_warmup_steps=1000 // gradient_accumulation_steps,\n",
    "    num_training_steps=num_training_steps)\n",
    "print(f\"num_training_steps: {num_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47941e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for check lr scheduler, which make scheduler empty, not run this cell when training\n",
    "# from matplotlib import pyplot as plt\n",
    "# lst = []\n",
    "# for _ in range(num_training_steps):\n",
    "#     lr_scheduler.step()\n",
    "#     lst.append(lr_scheduler.get_lr())\n",
    "# plt.plot(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998387b9",
   "metadata": {},
   "source": [
    "### 3.2 traininig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d62054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate the loss of the model\n",
    "@torch.no_grad()\n",
    "def evaluate_prompter(model, dataloader):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        document_input_ids = batch[\"document_input_ids\"].to(device)\n",
    "        document_attention_mask = batch[\"document_attention_mask\"].to(device)\n",
    "        prompter_question_input_ids = batch[\"prompter_question_input_ids\"].to(device)\n",
    "        prompter_question_attention_mask = batch[\"prompter_question_attention_mask\"].to(device)\n",
    "        prompter_output = prompter(\n",
    "            document_input_ids=document_input_ids,\n",
    "            document_attention_mask=document_attention_mask,\n",
    "            question_input_ids=prompter_question_input_ids,\n",
    "            question_attention_mask=prompter_question_attention_mask,\n",
    "        )\n",
    "        loss = prompter_output.loss.item()\n",
    "        losses.append(loss)\n",
    "    out[\"ranker_val_loss\"] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db2b5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10348/10348 [8:51:49<00:00,  3.08s/it] "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=num_training_steps)\n",
    "step = 0 # total steps = num_training_steps * gradient_accumulation_steps\n",
    "for epoch in range(num_epochs):\n",
    "    # Iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        document_input_ids = batch[\"document_input_ids\"].to(device)\n",
    "        document_attention_mask = batch[\"document_attention_mask\"].to(device)\n",
    "        prompter_question_input_ids = batch[\"prompter_question_input_ids\"].to(device)\n",
    "        prompter_question_attention_mask = batch[\"prompter_question_attention_mask\"].to(device)\n",
    "        prompter_output = prompter(\n",
    "            document_input_ids=document_input_ids,\n",
    "            document_attention_mask=document_attention_mask,\n",
    "            question_input_ids=prompter_question_input_ids,\n",
    "            question_attention_mask=prompter_question_attention_mask,\n",
    "        )\n",
    "        loss = prompter_output.loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            pbar.update(1)\n",
    "        step += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98edbf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2491f4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving checkpoint to saved_model/ranker\n"
     ]
    }
   ],
   "source": [
    "model_output_dir = Path(\"saved_model/ranker\")\n",
    "model_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "checkpoint = {\n",
    "    'model': prompter.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': prompter_config,\n",
    "    'iter_num': step,\n",
    "    'best_val_loss': None,\n",
    "    'config': None,\n",
    "}\n",
    "print(f\"saving checkpoint to {out_dir}\")\n",
    "torch.save(checkpoint, out_dir /'ckpt.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deb895b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = evaluate_prompter(prompter, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90537388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ranker_val_loss': 0.18672097759533285}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35a6b380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torchmetrics.retrieval import RetrievalRecall\n",
    "indexes = tensor([0, 0, 0, 1, 1, 1, 1])\n",
    "preds = tensor([0.2, 0.3, 0.5, 0.1, 0.3, 0.5, 0.2])\n",
    "target = tensor([False, False, True, False, True, False, True])\n",
    "r2 = RetrievalRecall(top_k=1)\n",
    "r2(preds, target, indexes=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004c807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
